[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a maths graduate who’s very interested in the many applications of mathematics and statistics to medicine, and I decided to start this website as a way of challenging myself to keep learning new things (as well as deepening my understanding of areas I already have some knowledge of). It’s a place where I can practise writing expository articles that explore various topics in ‘medical maths’ using diagrams and R code. Thank you for reading – I hope you find my explanations interesting and/or useful!\n\n(If you notice an error or have any other feedback, please feel free to contact me at: medmaths@protonmail.com)\nThis site is built with Quarto and hosted via Github Pages."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis article was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nPatients and medical professionals are often interested in the probability that an event will or will not have occurred after a certain amount of time. For example, what is the probability that a burn will have healed after two weeks? What is the probability that someone’s cancer will not have returned five years after they have gone into remission? To estimate these probabilities, researchers use a branch of statistics known as survival analysis."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#time-to-event-data",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#time-to-event-data",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "Time to Event Data",
    "text": "Time to Event Data\nSurvival analysis is the study of time to event data. These are data that for each subject measure the time that elapses from a defined starting point – for example, the diagnosis of a disease – until a particular event happens – for example, death. Despite the name, survival analysis does not presume that this event is death – indeed, it could be a positive event, such as discharge from hospital. Though commonly used in medical statistics, survival analysis is not exclusive to epidemiology. It is also employed in fields as diverse as engineering and sociology, meaning it is sometimes known by other names, such as ‘reliability analysis’1.\nOne of the central tasks of survival analysis is to estimate the survival function, \\(S(t)\\), for the event of interest. This is a function of time equal to the probability that the event takes longer than the input time, \\(t\\), to occur\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(T\\) denotes a random variable representing the time until the event occurs.\n\\(t\\) denotes a specific time duration.\n\\(S(t)=\\mathbb{P}(T&gt;t)\\) denotes the survival function.\n\n\n\nLet’s consider a very simple scenario in which we might estimate a survival function. Suppose an electrical manufacturing company has a batch of faulty light bulbs, and they decide to pick ten of them at random to leave on and monitor for a week. They observe that:\n\nOn the first day, three light bulbs fail.\nOn the second day, four light bulbs fail.\nOn the third day, no light bulbs fail\nOn the fourth day, two light bulbs fail.\nOn the fifth day, the final light bulb fails.\n\n\nPutting aside the fact that this is a very small sample, it is relatively straightforward to estimate the survival function for these faulty light bulbs from this experiment. For example, to estimate \\(S(1)\\), we just look at the proportion of light bulbs that last longer than one day – seven tenths. By the law of large numbers, as we increase the number of light bulbs in the sample this proportion will eventually converge to the true probability of a faulty light bulb lasting longer than one day. We can do this calculation for each time marker, including the start point of the experiment – we call this Day 0.\n\n  \n\nEstimated Faulty Light Bulb Survival Function\n\n\nDay\nSurvival Function Estimate\n\n\n0\n\\(1\\)\n\n\n1\n\\(\\frac{7}{10}\\)\n\n\n2\n\\(\\frac{3}{10}\\)\n\n\n3\n\\(\\frac{3}{10}\\)\n\n\n4\n\\(\\frac{1}{10}\\)\n\n\n5\n\\(0\\)\n\n\n\n\n\n\n\nPlotting the Estimated Survival Function in R\ndays &lt;- 0:5 # creating a vector of time markers\nsurvival_function &lt;- c(1, 7/10, 3/10, 3/10, 1/10, 0) # creating a vector of estimated probabilities\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nplot(days, survival_function, type=\"s\", ylim=c(0,1), xlab=\"Day\", ylab=\"Estimated Survival Function\", col=\"#006a90\", lwd=4.8, cex.lab=1.8, cex.axis=1.8)\n\n\n\n\n\n\n\n\n\nEasy, right? Unfortunately, the time to event data we find in medical applications are rarely this straightforward to work with."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#censoring",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#censoring",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "Censoring",
    "text": "Censoring\nIn many medical studies producing time to event data, patients are recruited over a period of some time and then followed until a defined end date, at which point the investigators will want to analyse their outcomes. As a result of this, the event of interest may not have happened for all the subjects of the study. In survival analysis, these ‘incomplete’ data points are known as censored data. They do give us some information – we know the event we are studying had not yet happened at the point at which censoring occurred. However, we cannot tell when, or indeed if, it would have gone on to happen.\nCensoring can also come about in other ways, including patients being lost to follow up, or experiencing a ‘competing event’2. For example, in a study of times from catheter placement to complication in acute peritoneal dialysis, observations were considered censored when a catheter was removed before a complication arose3.\nCensored data make estimating a survival function considerably more complicated. Imagine, for example, that three of the light bulbs in our previous thought experiment accidentally got smashed at the end of the first day.\n\nWe could still approximate \\(S(1)\\) to be seven tenths, since \\[\\mathbb{P}(T&gt;1)=1-\\mathbb{P}(T\\leq1),\\] which, since three light bulbs fail on the first day, we can estimate to be \\[1-\\frac{3}{10}=\\frac{7}{10},\\] as in the original case. But what about \\(S(2)\\) onwards? We don’t know on which days the smashed light bulbs would have failed, so we can’t estimate \\(S(t)\\) for these values of \\(t\\) in the same way."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#the-kaplan-meier-method",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#the-kaplan-meier-method",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "The Kaplan-Meier Method",
    "text": "The Kaplan-Meier Method\nIn 1958, Edward Kaplan and Paul Meier proposed an ingenious solution to the censoring problem4. Their paper has gone on to become the most cited publication in the history of statistics5, and is seen as a seminal contribution to the field of survival analysis. Kaplan and Meier’s central insight was that while censoring may stop us from directly estimating \\(\\mathbb{P}(T&gt;t_i)\\) for a particular value of time, \\(t_i\\), it does not stop us from estimating \\[\\mathbb{P}(T&gt;t_i\\,|\\,T&gt;t_{i-1}),\\] This is the conditional probability of an event taking longer than \\(t_i\\) days, months, or years to occur, given that it has not already happened in \\(t_{i-1}\\) days, months, or years.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(t_1,t_2,t_3\\ldots\\), etc. denote the times to the event of interest in ascending order.\n\\(t_{i-1}\\) denotes the next lowest time to the event of interest to \\(t_i\\).\n\\(t_0\\) denotes a time of \\(0\\).\n\n\n\nThese conditional probabilities are quite easy to find, since\n\\[\\begin{split}\n\\mathbb{P}(T&gt;t_i\\,|\\,T&gt;t_{i-1})&=1-\\mathbb{P}(T\\leq t_i\\,|\\,T&gt;t_{i-1})\\\\&=1-\\mathbb{P}(T=t_i\\,|\\,T&gt;t_{i-1}).\n\\end{split}\\]\nwhich we can estimate by calculating the proportion of subjects remaining in the study (those who have not experienced censoring or the event of interest) after \\(t_{i-1}\\) that do not have a time to event of \\(t_i\\).\nIn our smashed light blub thought experiment, for example, we could estimate \\(\\mathbb{P}(T&gt;2\\,|\\,T&gt;1)\\) to be two quarters (or one half). This is because on the first day three light bulbs fail and three light bulbs are censored, so four remain in the experiment on the second day, two of which survive\nKaplan and Meier realised that we can get from these conditional probabilities to the survival function using the chain rule of conditional probabilities. For any \\(t_i\\), we have that\n\\[\n\\begin{split}\n\\mathbb{P}(T&gt;t_i)\n&=\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\mathbb{P}(T&gt;t_{i-1})\\\\\n&=\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\mathbb{P}(T&gt;t_{i-1}\\,|\\,T&gt;t_{i-2})\\mathbb{P}(T&gt;t_{i-2})\\\\\n&=\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\mathbb{P}(T&gt;t_{i-1}\\,|\\,T&gt;t_{i-2})\\mathbb{P}(T&gt;t_{i-2}\\,|\\,T&gt;t_{i-3})\\mathbb{P}(T&gt;t_{i-3})\\cdots\\\\\n&\\cdots\\mathbb{P}(T&gt;t_2\\,|\\,T&gt;t_1)\\mathbb{P}(T&gt;t_2\\,|\\,T&gt;t_1)\\mathbb{P}(T&gt;t_1\\,|\\,T&gt;t_0)\\mathbb{P}(T&gt;t_0).\n\\end{split}\n\\] We know that \\(\\mathbb{P}(T&gt;t_0)\\), the probability of the time to the event of interest being more than zero, is one, since the event has not happened to any of our subjects at the start of the study. So by the expression above, we can recursively estimate the survival function \\(S(t_{i})\\) for each \\(t_1,t_2,t_3,\\ldots\\), using the cumulative product of the conditional probabilities \\(\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\)."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#an-example-with-real-data",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#an-example-with-real-data",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "An Example With Real Data",
    "text": "An Example With Real Data\nTo get an idea of how the Kaplan-Meier method of estimating a survival function works, let’s try it out on some data from a real observational study from the 1980s6. Over two years, a group of obstetricians in London recorded the time it took for a group of 38 women who had been experiencing fertility issues to conceive after a surgical intervention. The times to conception or censoring for each woman can be obtained from a 1998 article on the Kaplan-Meier method in the BMJ7.\n\n\nInputting the Results of the Study into a Data Frame in R\nwomen &lt;- 1:38 # creating identifiers for each woman in the study\nevent_times &lt;- c(1, 1, 11, 7, 3, 4, 9, 4, 24, 6, 9, 2, 6, 16, 9, 2, 2, 3, 1, 8, 8, 4, 4, 1, 7, 9, 2, 9, 24, \n                 1, 1, 9, 13, 10, 3, 2, 3, 2) # inputting the number of months after their surgical intervention\n                                              # at which they conceived or were censored\nconception_or_censoring &lt;- c('Conceived', 'Conceived', 'Censored', 'Censored', 'Conceived', 'Conceived', 'Censored', 'Conceived', 'Censored', 'Conceived', 'Censored',\n                             'Conceived', 'Conceived', 'Conceived', 'Conceived', 'Censored', 'Conceived', 'Conceived', 'Conceived', 'Censored', 'Censored', 'Conceived',\n                             'Censored', 'Conceived', 'Censored', 'Conceived', 'Conceived', 'Conceived', 'Censored', 'Conceived', 'Conceived', 'Censored', 'Conceived',\n                             'Conceived', 'Conceived', 'Conceived', 'Censored', 'Conceived') # inputting which event occurred for each woman\nconception_study &lt;- data.frame(subject=women, months=event_times, event=conception_or_censoring)\n\n\n\n  \n\nTime to Event Data from a Study of Conception after Laparoscopy and Hydrotubation\n\n\n\nStudy Participant\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\nEvent\nConceived\nConceived\nCensored\nCensored\nConceived\nConceived\nCensored\nConceived\nCensored\nConceived\nCensored\nConceived\nConceived\nConceived\nConceived\nCensored\nConceived\nConceived\nConceived\nCensored\nCensored\nConceived\nCensored\nConceived\nCensored\nConceived\nConceived\nConceived\nCensored\nConceived\nConceived\nCensored\nConceived\nConceived\nConceived\nConceived\nCensored\nConceived\n\n\nMonths Until Event\n1\n1\n11\n7\n3\n4\n9\n4\n24\n6\n9\n2\n6\n16\n9\n2\n2\n3\n1\n8\n8\n4\n4\n1\n7\n9\n2\n9\n24\n1\n1\n9\n13\n10\n3\n2\n3\n2\n\n\n\n\n\nTo make it easier to do our survival analysis, let’s summarise this data so that we can easily see the number of women who conceived or were censored at each distinct time step.\n\n\nRestructuring the Data in R\nmonths_from_intervention &lt;- sort(unique(conception_study$months)) # getting the different numbers of months after surgical intervention \n                                                                  # it took for conception or censoring to occur\nmonths_from_intervention &lt;- c(0, months_from_intervention) # adding month zero for the start of the study\nnum_times &lt;- length(months_from_intervention) # finding the number of unique event times \ncensored &lt;- rep(NA, times=num_times) \nconceived &lt;- rep(NA, times=num_times)\nfor (i in 1:num_times) { # counting the number of conceptions and censorings at each event time\n  month_i &lt;- months_from_intervention[i]\n  censored[i] &lt;- nrow(conception_study[conception_study$months == month_i & conception_study$event == 'Censored', ])\n  conceived[i] &lt;- nrow(conception_study[conception_study$months == month_i & conception_study$event == 'Conceived', ])\n}\nconception_data_summary &lt;- data.frame(months=months_from_intervention, censored=censored, conceived=conceived)\n\n\n\n  \n\nSummarised Conception Study Data\n\n\nMonths from Intervention\nNumber of Censorings\nNumber of Conceptions\n\n\n0\n0\n0\n\n\n1\n0\n6\n\n\n2\n1\n5\n\n\n3\n1\n3\n\n\n4\n1\n3\n\n\n6\n0\n2\n\n\n7\n2\n0\n\n\n8\n2\n0\n\n\n9\n3\n3\n\n\n10\n0\n1\n\n\n11\n1\n0\n\n\n13\n0\n1\n\n\n16\n0\n1\n\n\n24\n2\n0\n\n\n\n\n\nTo calculate the conditional probability \\(\\mathbb{P}(T&gt;t_i|T&gt;t_{i-1})\\) for each observed number of months from conception, \\(t_i\\), we first need to find out how many women were still being followed up at each stage.\n\n\nFinding the Number of Remaining Study Members at Each Time Stage in R\nnum_remaining &lt;- rep(NA, times=num_times) # initialising a vector to store the number of remaining study members at each event time\nnum_remaining[1] &lt;- 38 # at month zero, all women are still being followed up \nfor (i in 2:num_times) { # finding the number of women still being followed up at each event time from the previous number, \n                         # and the number who conceived or were censored at the previous event time\n  num_remaining[i] &lt;- num_remaining[i-1] - censored[i-1] - conceived[i-1]\n}\nconception_data_summary$remaining &lt;- num_remaining # adding a column to the data frame\n\n\n\n  \n\nSummarised Conception Study Data (Including Numbers of Participants Remaining in Follow Up)\n\n\nMonths from Intervention\nNumber of Censorings\nNumber of Conceptions\nNumber Still in Study\n\n\n0\n0\n0\n38\n\n\n1\n0\n6\n38 − 0 − 0 = 38\n\n\n2\n1\n5\n38 − 0 − 6 = 32\n\n\n3\n1\n3\n32 − 1 − 5 = 26\n\n\n4\n1\n3\n26 − 1 − 3 = 22\n\n\n6\n0\n2\n22 − 1 − 3 = 18\n\n\n7\n2\n0\n18 − 0 − 2 = 16\n\n\n8\n2\n0\n16 − 2 − 0 = 14\n\n\n9\n3\n3\n14 − 2 − 0 = 12\n\n\n10\n0\n1\n12 − 3 − 3 = 6\n\n\n11\n1\n0\n6 − 0 − 1 = 5\n\n\n13\n0\n1\n5 − 1 − 0 = 4\n\n\n16\n0\n1\n4 − 0 − 1 = 3\n\n\n24\n2\n0\n3 − 0 − 1 = 2\n\n\n\n\n\nNow we can start our survival analysis properly by estimating \\(\\mathbb{P}(T&gt;t_i\\,|\\,T&gt;t_{i-1})\\) for each \\(t_i\\). We have gleaned all the information we need from the number of censorings at each time stage, so this column can be removed.\n\n\nEstimating the Conditional Probabilities for Each Time Stage in R\nconception_survival_analysis &lt;- subset(conception_data_summary, select=-censored) # creating a new data frame without the censoring column\nconditional_probabilities &lt;- rep(NA, times=num_times) # initialising a vector to store the estimated conditional probabilities\nfor (i in 1:num_times) { # finding an estimate for the conditional probability at each event time\n  conditional_probabilities[i] &lt;- (num_remaining[i]-conceived[i])/num_remaining[i]\n}\nconception_survival_analysis$conditionals &lt;- conditional_probabilities\n\n\n\n  \n\nConception Study Data With Estimated Conditional Probabilities\n\n\n\\(i\\)\n\\(t_{i}\\) (Months)\n\\(\\#(T = t_{i})\\) (Conceptions)\nNumber Still in Study\nEstimated \\(\\mathbb{P}(T &gt; t_{i}\\,|\\, T &gt; t_{i - 1})\\)\n\n\n0\n0\n0\n38\n\\(\\text{N/A}\\,(\\mathbb{P}(T &gt; 0) = 1)\\)\n\n\n1\n1\n6\n38\n\\(1 - \\frac{6}{38} = \\frac{32}{38} \\approx 0.84\\)\n\n\n2\n2\n5\n32\n\\(1 - \\frac{5}{32} = \\frac{27}{32} \\approx 0.84\\)\n\n\n3\n3\n3\n26\n\\(1 - \\frac{3}{26} = \\frac{23}{26} \\approx 0.88\\)\n\n\n4\n4\n3\n22\n\\(1 - \\frac{3}{22} = \\frac{19}{22} \\approx 0.86\\)\n\n\n5\n6\n2\n18\n\\(1 - \\frac{2}{18} = \\frac{16}{18} \\approx 0.89\\)\n\n\n6\n7\n0\n16\n\\(1 - \\frac{0}{16} = 1\\)\n\n\n7\n8\n0\n14\n\\(1 - \\frac{0}{14} = 1\\)\n\n\n8\n9\n3\n12\n\\(1 - \\frac{3}{12} = \\frac{9}{12} = 0.75\\)\n\n\n9\n10\n1\n6\n\\(1 - \\frac{1}{6} = \\frac{5}{6} \\approx 0.83\\)\n\n\n10\n11\n0\n5\n\\(1 - \\frac{0}{5} = 1\\)\n\n\n11\n13\n1\n4\n\\(1 - \\frac{1}{4} = \\frac{3}{4} = 0.75\\)\n\n\n12\n16\n1\n3\n\\(1 - \\frac{1}{3} = \\frac{2}{3} \\approx 0.67\\)\n\n\n13\n24\n0\n2\n\\(1 - \\frac{0}{2} = 1\\)\n\n\n\n\n\nFinally, we can use these conditional probabilities to estimate \\(S(t)\\).\n\n\nEstimating the Survival Function in R\nsurvival_function &lt;- cumprod(conditional_probabilities) # finding the cumulative product of the estimated conditional probabilities\nconception_survival_analysis$survival &lt;- survival_function\n\n\n\n  \n\nSurvival Analysis of Time to Event Data from a Study of Conception after Laparoscopy and Hydrotubation\n\n\n\\(i\\)\n\\(t_{i}\\) (Months)\n\\(\\#(T = t_{i})\\) (Conceptions)\nNumber Still in Study\nEstimated \\(\\mathbb{P}(T &gt; t_{i}\\,|\\, T &gt; t_{i - 1})\\)\nEstimated \\(\\mathbb{P}(T &gt; t_{i})\\)\n\n\n0\n0\n0\n38\n\\(\\text{N/A}\\,(\\mathbb{P}(T &gt; 0) = 1)\\)\n\\(1\\)\n\n\n1\n1\n6\n38\n\\(\\frac{32}{38}\\)\n\\(\\frac{32}{38} \\times 1 = \\frac{32}{38} \\approx 0.84\\)\n\n\n2\n2\n5\n32\n\\(\\frac{27}{32}\\)\n\\(\\frac{27}{32} \\times \\frac{32}{38} = \\frac{27}{38} \\approx 0.71\\)\n\n\n3\n3\n3\n26\n\\(\\frac{23}{26}\\)\n\\(\\frac{23}{26} \\times \\frac{27}{38} = \\frac{621}{988} \\approx 0.63\\)\n\n\n4\n4\n3\n22\n\\(\\frac{19}{22}\\)\n\\(\\frac{19}{22} \\times \\frac{621}{988} = \\frac{621}{1144} \\approx 0.54\\)\n\n\n5\n6\n2\n18\n\\(\\frac{16}{18}\\)\n\\(\\frac{16}{18} \\times \\frac{621}{1144} = \\frac{69}{143} \\approx 0.48\\)\n\n\n6\n7\n0\n16\n\\(1\\)\n\\(1 \\times \\frac{69}{143} = \\frac{69}{143} \\approx 0.48\\)\n\n\n7\n8\n0\n14\n\\(1\\)\n\\(1 \\times \\frac{69}{143} = \\frac{69}{143} \\approx 0.48\\)\n\n\n8\n9\n3\n12\n\\(\\frac{9}{12}\\)\n\\(\\frac{9}{12} \\times \\frac{69}{143} = \\frac{207}{572} \\approx 0.36\\)\n\n\n9\n10\n1\n6\n\\(\\frac{5}{6}\\)\n\\(\\frac{5}{6} \\times \\frac{207}{572} = \\frac{345}{1144} \\approx 0.30\\)\n\n\n10\n11\n0\n5\n\\(1\\)\n\\(1 \\times \\frac{345}{1144} = \\frac{345}{1144} \\approx 0.30\\)\n\n\n11\n13\n1\n4\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4} \\times \\frac{345}{1144} = \\frac{1035}{4576} \\approx 0.23\\)\n\n\n12\n16\n1\n3\n\\(\\frac{2}{3}\\)\n\\(\\frac{2}{3} \\times \\frac{1035}{4576} = \\frac{345}{2288} \\approx 0.15\\)\n\n\n13\n24\n0\n2\n\\(1\\)\n\\(1 \\times \\frac{345}{2288} = \\frac{345}{2288} \\approx 0.15\\)\n\n\n\n\n\nWe can now plot our estimate for \\(\\mathbb{P}(T&gt;t)\\) as a stepwise function of \\(t\\). As is conventional when plotting Kaplan-Meier curves we also mark the months in which censoring occurs.\n\n\nPlotting the Estimated Survival Function in R\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nKM_curve_plot &lt;- function (surv_df, study_df) {\n  plot(surv_df$months, surv_df$survival, type=\"s\", ylim=c(0,1), \n       xlab=\"Months\", ylab=\"Estimated Survival Function\", col=\"#009ed8\", lwd=4.8, cex.lab=1.8, cex.axis=1.8)\n  censoring_months &lt;- sort(unique(study_df[which((study_df$event == 'Censored')),]$months))  # finding the months in which censoring occurred\n  surv_fuc_eval_at_censoring &lt;- surv_df[surv_df$months %in% censoring_months,]$survival # evaluating the estimated survival function\n                                                                                        # at those points\n  points(censoring_months, surv_fuc_eval_at_censoring, pch=4, cex=1.8, col=\"#006a90\", lwd=4.8)\n}\nKM_curve_plot(conception_survival_analysis, conception_study)\n\n\n\n\n\n\n\n\n\nOur estimated survival function allows us to work out a principled estimate for the median time to conception after laparoscopy and hydrotubation. Since the median time it takes to conceive, \\(t_m\\), is equal to the point where \\(\\mathbb{P}(T&gt;t_m)=0.5\\), we can just draw a line across from 0.5 on the y-axis to our Kaplan-Meier curve.\n\n\nFinding the Median Time to Conception in R\nmedian_month &lt;- conception_survival_analysis$months[which(conception_survival_analysis$survival &lt;= 0.5)[1]] # finding the first month at which the estimated survival function drops to equal or below 0.5\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nKM_curve_plot(conception_survival_analysis, conception_study)\nsegments(-1, 0.5, median_month, 0.5, col=\"#ffc000\", lwd=4.8, lty=2) # plotting the line across from the y-axis to the survival curve\naxis(1, at=median_month, cex.lab=1.8, cex.axis=1.8) # marking 0.5 on the y-axis\nsegments(median_month, -1, median_month, 0.5, col=\"#ffc000\", lwd=4.8, lty=2) # plotting the line down from the survival curve to the x-axis\naxis(2, at=0.5, cex.lab=1.8, cex.axis=1.8) # marking the median on the x-axis\n\n\n\n\n\n\n\n\n\nWe find that the median number of months that passed before women conceived after laparoscopy and hydrotubation was six. So, based on the results of this study, we would expect a woman to have a 50% chance of conceiving within the first six months following laparoscopy and hydrotubation."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#further-reading",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#further-reading",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "Further Reading",
    "text": "Further Reading\n\nWhat is the Kaplan Meier curve? – a blog post by Steve Simon\nWhat is survival analysis? – a blog post by Antoine Soetewey"
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#footnotes",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#footnotes",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "References",
    "text": "References\n\n\nEmily Zabor, “Survival Analysis in R,” June 21, 2023.↩︎\nDouglas Altman and Martin Bland, “Time to event (survival) data,” BMJ 317, no. 7156 (August 1998): 468–469.↩︎\nVimal Chadha et al., “Tenckhoff catheters prove superior to cook catheters in pediatric acute peritoneal dialysis,” American Journal of Kidney Diseases 35, no. 6 (June 2000): 111–1116.↩︎\nEdward Kaplan and Paul Meier, “Nonparametric Estimation From Incomplete Observations,” Journal of the American Statistical Association 53, no. 282 (June 1958): 45–481.↩︎\nRichard Van Noorden, Brendan Maher, and Regina Nuzzo, “The Top 100 Papers,” Nature 514, no. 7524 (October 2014): 55–553.↩︎\nParamjit Luthra, Martin Bland, and Stuart Stanton, “Incidence of Pregnancy After Laparoscopy and Hydrotubation,” British Medical Journal (Clinical Research Edition) 284, no. 6321 (April, 1982): 101.↩︎\nMartin Bland and Douglas Altman, “Survival probabilities (the Kaplan-Meier method),” BMJ 317, no. 7172 (December 1998): 1572–1580.↩︎"
  },
  {
    "objectID": "posts/kidney-transplants-and-graph-theory/index.html",
    "href": "posts/kidney-transplants-and-graph-theory/index.html",
    "title": "Kidney Transplants and Graph Theory",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis article was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nThousands of kidney transplants are performed every year in the UK1, transforming the lives of their recipients2. In this article, I’ll explore how techniques from the mathematical discipline of graph theory help to facilitate some of these transplants."
  },
  {
    "objectID": "posts/kidney-transplants-and-graph-theory/index.html#some-graph-theory-background",
    "href": "posts/kidney-transplants-and-graph-theory/index.html#some-graph-theory-background",
    "title": "Kidney Transplants and Graph Theory",
    "section": "Some Graph Theory Background",
    "text": "Some Graph Theory Background\nSuppose that you are a teacher in charge of taking a group of six students, who we will call Students A, B, C, D, E, and F, on a school trip, and you need to decide what pairs they are going to sit in on the minibus. You ask the students to anonymously let you know who they would be willing to sit with and find out that:\n\nStudent A and Student D would be happy to be in a pair.\nStudent A and Student E would be happy to be in a pair.\nStudent A and Student F would be happy to be in a pair.\nStudent B and Student C would be happy to be in a pair.\nStudent B and Student E would be happy to be in a pair.\nStudent C and Student D would be happy to be in a pair.\n\nWe can represent this information in a graph, which, in discrete mathematics, means a structure consisting of vertices and edges (as opposed to describing a chart or a plot).\n\nHere, the points of the graph – their vertices (sometimes also called nodes) – represent the students, and the lines between the vertices, known as edges, encode the possible pairings they can sit in; if two students would accept being seated together, they have an edge between them. Graphs are commonly used to study community structures in sociology, and they can also be applied to protein-protein interaction networks in biology and transportation networks in urban planning, to give just two other examples.\nGraphs can be represented computationally in several different ways; here, we will discuss three of the most common – adjacency matrices, adjacency lists, and edge lists. For a graph \\(G\\) with \\(n\\) vertices, the adjacency matrix of \\(G\\) is an \\(n\\times n\\) matrix \\(A_G\\), where the entry in row \\(i\\) and column \\(j\\) of \\(A_G\\) is \\(1\\) if vertex \\(i\\) and vertex \\(j\\) are connected by an edge, and \\(0\\) if they are not. For the rest of this article, we will denote the specific graph structure above, representing the possible seating arrangements of the students, by \\(S\\). The adjacency matrix for \\(S\\) is\n\\[\nA_S=\n\\begin{pmatrix}\n0 & 0 & 0 & 1 & 1 & 1 \\\\\n0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0\n\\end{pmatrix}.\n\\]\n\n\nDefining the Adjacency Matrix for \\(S\\) in R\nS_adj_mat &lt;- matrix(c(# Let's walk through how we fill out the first row of the adjacency matrix:\n                      # Row 1 represents the people Student A can sit with; \n                      # they form a compatible pair with Students D, E, and F,\n                      # so there is a 1 in column 4, representing Student D,\n                      # a 1 in column 5, representing Student E,\n                      # and a 1 in column 6, representing Student F. \n                      # The pairings A-B and A-C are unacceptable so there are 0s in columns 2 and 3. \n                      # Vertex A is not connected to itself, because Student A cannot form a pair with themselves,\n                      # so there is a 0 in column 1 – in fact, all the diagonal values of this adjacency matrix are 0\n                      # because none of the vertices have edges connecting them to themselves.\n                      0, 0, 0, 1, 1, 1, \n                      0, 0, 1, 0, 1, 0,\n                      0, 1, 0, 1, 0, 0,\n                      1, 0, 1, 0, 0, 0,\n                      1, 1, 0, 0, 0, 0,\n                      1, 0, 0, 0, 0, 0), \n                      nrow = 6, byrow = TRUE)\n\n\nAs well as an adjacency matrix \\(M\\) for \\(G\\), we can also create an adjacency list, \\(L_G\\), which is a list of length \\(n\\), where each entry is also a list. The \\(i\\)th element of \\(L_G\\) is a list of vertices that vertex \\(i\\) of graph \\(G\\) forms an edge with – this list can have a length of zero if vertex \\(i\\) is not connected to any vertices.\n\n\nFinding \\(L_S\\) from \\(A_S\\) in R\n# Let's write a function which takes as its input a graph's adjacency matrix, and returns the adjacency list for the graph \nadj_mat_to_adj_list &lt;- function(adj_mat){\n  num_nodes &lt;- ncol(adj_mat) # we find the number of vertices of the graph\n  adjacency_list &lt;- vector(mode = \"list\", length = num_nodes) # we initialise our adjacency list as an empty list with the same number of elements of vertices of the graph\n  # Now we make an empty list for each vertex and fill it with the indexes of the vertices that it is connected to:\n  for (i in 1:num_nodes){\n    connected_vertices &lt;- list()\n    for (j in 1:num_nodes) {\n      if (adj_mat[i,j] == 1) {\n        connected_vertices &lt;- c(connected_vertices, j)\n      }\n    }\n    adjacency_list[[i]] &lt;- connected_vertices\n  }\n  adjacency_list\n}\n# We can use this function to find the adjacency list for S:\nS_adj_list &lt;- adj_mat_to_adj_list(S_adj_mat)\nnames(S_adj_list) &lt;- c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\")\ncat(\"Vertex 1 (A):\", LETTERS[unlist(S_adj_list$A)], \n    \"\\nVertex 2 (B):\", LETTERS[unlist(S_adj_list$B)],\n    \"\\nVertex 3 (C):\", LETTERS[unlist(S_adj_list$C)],\n    \"\\nVertex 4 (D):\", LETTERS[unlist(S_adj_list$D)],\n    \"\\nVertex 5 (E):\", LETTERS[unlist(S_adj_list$E)],\n    \"\\nVertex 6 (F):\", LETTERS[unlist(S_adj_list$F)])\n\n\nVertex 1 (A): D E F \nVertex 2 (B): C E \nVertex 3 (C): B D \nVertex 4 (D): A C \nVertex 5 (E): A B \nVertex 6 (F): A\n\n\nFinally, we can also create an ‘edge list’, \\(E_G\\), of all the edges of \\(G\\).\n\n\nFinding \\(E_S\\) from \\(A_S\\) in R\n# We can write a simple function to get the edge list matrix of a graph from it's adjacency matrix:\nadj_mat_to_edge_list &lt;- function(adj_mat) {\n  # We can represent each edge as a vector of length two where the elements are the vertices\n  # that the edge is connected to, and thus encode the edge list as a two column matrix\n  edge_list &lt;- unique(t(apply(which(adj_mat == 1, arr.ind = TRUE), 1, sort))) \n  edge_list\n}\n# Let's see what the output looks like for S:\nS_edge_list &lt;- adj_mat_to_edge_list(S_adj_mat)\n# We can convert the vertex numbers to letters to check we've got the result we've expected:\nletters_edge_list &lt;- apply(S_edge_list, 1:2, function(i) LETTERS[i])\nfor (i in 1:nrow(letters_edge_list)) {\n  cat(letters_edge_list[i,1], \"–\", letters_edge_list[i,2], \",\\n\", sep = \"\")\n}\n\n\nA–D,\nA–E,\nA–F,\nB–C,\nB–E,\nC–D,\n\n\nNow that we know some definitions, we can return to our problem – finding a configuration in which the maximum possible number of passengers on the minibus are happy with their partner. Each person can only sit with one other person, so in graph theory terms we are looking to find a matching of the graph – this is a set of edges of the graph in which no two edges share any common vertices3. But it’s not enough just to find any matching – for example the set of edges \\(\\{\\)A–D\\(\\}\\) is a matching, as is the set \\(\\{\\)C–D\\(\\}\\), but neither are very useful to us as they both only give us one pairing.\n\nInstead, we are looking for a maximum matching, which is a matching of a graph that has the largest possible number of edges. One way to find a maximum matching of a graph \\(G\\) would be to check all the possible subsets of edges of \\(G\\), throw out the ones that aren’t matchings, and then pick the largest remaining subset. But since the set of all possible subsets of edges is equal to the power set of the set of edges of \\(G\\), it contains \\(2^{E}\\) subsets for us to check, where \\(E\\) is the number of edges of \\(G\\). For our graph \\(S\\), this would give us \\(2^6=64\\) subsets to check, which might just about be manageable, but as the number of edges increases, the number of subsets we will need to check using this naive approach will grow exponentially. A graph with 12 edges, for example, would require us to check \\(2^{12}=4096\\) subsets!\n\n\nPlotting the Number of Subsets of Edges Against the Number of Edges in R\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nxs &lt;- 1:12\nys &lt;- 2^xs\nplot(xs, ys, pch=4, cex=1.8, lwd=4.8, col=\"#009ed8\",\n     xlab=\"Number of Edges of Graph\", ylab=\"Number of Subsets to Check\",\n     cex.lab=1.5, cex.axis=1.5)\n\n\n\n\n\n\n\n\n\nTo understand the algorithms that allow us to find a maximum matching more quickly, we need another concept from graph theory – augmenting paths. Given a graph \\(G\\), and a matching \\(M\\), an augmenting path is a sequence of edges, \\(P\\), that connect two unmatched vertices of \\(G\\) such that the edges \\(P\\) are alternately in \\(M\\) and not in \\(M\\). For example, if we have graph with four vertices and an edge list 1–2, 2–3, 3–4 and we are given a matching 2–3, the augmenting path is 1–2, 2–3, 3–4.\n\n\n\nWriting a Function that Will Find an Augmenting Path for a Given Graph and Matching in R\naugmenting_path &lt;- function(adj_list, match_vec) {\n  # This function takes as input the edge list of the graph, and a vector, `match_vec` which represents the matching. \n  # The length of `match_vec` should be the same as the number of vertices of the graph, and `match_vec[i]` is the index\n  # of the vertex that the $i$th vertex is matched to – if it is unmatched then we'll define `match_vec[i]` to be zero.\n  vertices_match_status &lt;- match_vec != 0 # we create a logical vector to represent whether each vertex is included in the matching\n  num_vertices &lt;- length(vertices_match_status) # we find the number of vertices in the graph\n  unmatched_vertices &lt;- which(vertices_match_status == FALSE, arr.ind=TRUE) # we create a vector of the vertices which are not in the matching\n  for (i in 1:num_vertices) { # we will try finding an augmenting path starting from each unmatched vertex until we find one\n    aug_path &lt;- matrix(, nrow=0, ncol=2) # we initialise an empty matrix to start the augmenting path \n    if (vertices_match_status[i] == TRUE) { # we skip the matched vertices\n      next \n    }\n    root &lt;- i\n    current_vert &lt;- i # we will use this to keep track of which vertex we have reached in our attempt to find an augmenting path\n    # Let's keep track of the vertices we've included in our augmenting path attempt from this vertex:\n    unvisited_from_root &lt;- c(1:num_vertices)\n    unvisited_from_root &lt;- unvisited_from_root[!unvisited_from_root==root]\n    repeat { # we create a repeating loop that will run the same procedure until we break out of it using break or return\n      neighbours &lt;- unlist(adj_list[[current_vert]]) # we create a vector of the vertices that have an edge between them and the vertex that we are currently on \n      neighbours_unvisited_from_root &lt;- intersect(neighbours, unvisited_from_root) # we create a vector of vertices adjacent to the vertex we are currently on that have not already been included in our attempt to find an augmenting path\n      if (length(neighbours_unvisited_from_root) == 0) { # if there are none of these, we can stop running our repear loop and try our searh again from the next unmatched vertex\n        break\n      }\n      match_to_current_vert &lt;- match_vec[current_vert] \n      # if we are on a vertex that is part of the matching, and the vertex it is matched to is not yet part of the augmenting path, we want to go to that vertex next \n      if (match_to_current_vert %in% unvisited_from_root) { \n        next_vert &lt;- match_to_current_vert\n        unvisited_from_root &lt;- unvisited_from_root[!unvisited_from_root==next_vert]\n        aug_path &lt;- rbind(aug_path, c(current_vert,next_vert))\n        current_vert &lt;- next_vert\n        next\n      }\n      unmatched_neighbours_unvisited_from_root &lt;- intersect(unmatched_vertices, neighbours_unvisited_from_root) # we create a vector of unmatched vertices adjacent to the vertex we are currently on that we haven't already included in our augmenting path attempt\n      if (length(unmatched_neighbours_unvisited_from_root) &gt; 0) { # if any of these exist, then we have found an augmenting path, and we can return it!\n        aug_path &lt;- rbind(aug_path, c(current_vert,unmatched_neighbours_unvisited_from_root[1]))\n        return(aug_path)\n      }\n      # If we didn't return an augmenting path in the if statement above, all the vertices adjacent to the vertex we are currently on that are not already included in our augmenting path attempt must be part of the matching, so we will go to one of these next and continue searching:\n      next_vert &lt;- neighbours_unvisited_from_root[1]\n      unvisited_from_root &lt;- unvisited_from_root[!unvisited_from_root==next_vert]\n      aug_path &lt;- rbind(aug_path, c(current_vert,next_vert))\n      current_vert &lt;- next_vert\n    }\n  }\n  # If we haven't returned an augmenting path after searching from all unmatched vertices, we return an empty matrix:\n  aug_path &lt;- matrix(, nrow=0, ncol=2)\n  return(aug_path) \n}\n\n\nHow do augmenting paths help us find the maximum matching of a graph? Well in 1957, mathematician Claude Berge proved that if an augmenting path cannot be found for a graph and a matching, then that matching is a maximum matching4. This is the insight that is central to the Hungarian Maximum Matching Algorithm, which works by starting with any matching on a graph (including an empty matching) and searching the graph from each unmatched vertex until it finds an augmenting path. It then finds the symmetric difference of the path and the current matching, and this set of edges becomes the new, improved matching that it will use for its next iteration. For example, in the scenario above where we have a graph with edge list 1–2, 2–3, 3–4 and a matching 2–3, the first iteration of the algorithm will find the augmenting path 1–2, 2–3, 3–4, and will update the matching to be 1–2, 3–4.\n\nWhen it can no longer find any augmenting paths, the Hungarian Maximum Matching Algorithm terminates, and thanks to Claude Berge, we can be sure that the matching it has found is a maximum matching.\n\n\nCoding the Hungarian Maximum Matching Algorithm in R\n# Let's use our augmenting_path function to write a Hungarian Maximum Matching Algorithm that takes as its input the adjacency matrix of a graph and a matching vector:\nmaximum_matching &lt;- function(adj_mat, match_vec) {\n  adj_list &lt;- adj_mat_to_adj_list(adj_mat) # we use the function we wrote earlian \n  num_vertices &lt;- length(match_vec) # we find the number of vertices in the graph\n  # Let's create a matrix to store the current matching:\n  current_matching &lt;- matrix(, nrow=0, ncol=2)\n  # We can populate it with the initial matching to start with:\n  for (i in 1:num_vertices) {\n    if (match_vec[i] == 0) {\n      next\n    }\n    current_matching &lt;- rbind(current_matching, c(i,match_vec[i]))\n  }\n  if (nrow(current_matching) != 0) {\n    current_matching &lt;- unique(t(apply(current_matching, 1, sort))) # we order the edges with the lower indexed vertex first, and remove duplicates\n  }\n  # Now we will run our augmenting path algorithm and and use it to increase our current matching until no more augmenting paths can be found:\n  current_match_vec &lt;- match_vec\n  repeat {\n    aug_path &lt;- augmenting_path(adj_list, current_match_vec) # we find an augmenting path for the input graph and the current matching \n    aug_path_length &lt;- nrow(aug_path)\n    # If the augmenting_path function has not found any augmenting paths, we break out of the repeat loop:\n    if (aug_path_length == 0) {\n      break \n    }\n    aug_path &lt;- t(apply(aug_path, 1, sort)) # we order the edges in the augmenting path with the lowest indexed vertex first\n    # If the augmenting path consists of only one edge, we add that edge to our matching and run another iteration of the augmenting_path function with the new matching:\n    if (aug_path_length == 1) { \n      current_matching &lt;- rbind(current_matching, aug_path) \n      # Before running the augmenting_path function again we need to update the current_match_vec algorithm to reflect our new matching:\n      current_matching_length &lt;- nrow(current_matching)\n      current_match_vec &lt;- rep(0,num_vertices)\n      for (i in 1:current_matching_length) {\n        current_match_vec[current_matching[i,1]] &lt;- current_matching[i,2]\n        current_match_vec[current_matching[i,2]] &lt;- current_matching[i,1]\n      }\n      next\n    }\n    # If the augmenting path has more than one edge, we add the edges that are not included in the current matching, and remove the ones that are \n    # (by the definition of an augmenting path, these alternate):\n    for (i in 1:aug_path_length) {\n      if (i%%2 == 1) { \n        current_matching &lt;- rbind(current_matching, aug_path[i,])\n      }\n      if (i%%2 == 0) {\n        current_matching &lt;- current_matching[!(current_matching[,1]==aug_path[i,1] & current_matching[,2]==aug_path[i,2]),]\n      }\n    }\n    # Before running the augmenting_path function again we need to update the current_match_vec algorithm to reflect our new matching:\n    current_matching_length &lt;- nrow(current_matching)\n    current_match_vec &lt;- rep(0,num_vertices)\n    for (i in 1:current_matching_length) {\n      current_match_vec[current_matching[i,1]] &lt;- current_matching[i,2]\n      current_match_vec[current_matching[i,2]] &lt;- current_matching[i,1]\n    }\n  }\n  return(current_matching[order(current_matching[,1]),]) # we return the first matching we arrived at for which no augmenting paths could be found,\n                                                         # with the edges ordered by their lowest indexed vertex\n}\n\n\nWhat does this algorithm give us when applied to our example graph \\(S\\), and an empty matching?\n\n\nApplying the Hungarian Maximum Matching Algorithm to \\(S\\) in R\nS_empty_matching &lt;- rep(0,6)\nS_max_matching &lt;- maximum_matching(S_adj_mat, S_empty_matching)\nletters_matching &lt;- apply(S_max_matching, 1:2, function(i) LETTERS[i])\nfor (i in 1:nrow(letters_matching)) {\n  cat(letters_matching[i,1], \"–\", letters_edge_list[i,2], \",\\n\", sep = \"\")\n}\n\n\nA–D,\nB–E,\nC–F,\n\n\nSo students A and F, B and E, and C and D, can all sit in pairs.\n\nThis may seem like something we could have worked out easily by hand, but matching algorithms are really useful when there are so many vertices and edges to a graph that a human could never work out a maximum matching just from looking at it.\nYou may be relieved to learn that we are nearly at the end of the background graph theory I wanted to include in this article before discussing matching algorithms’ applications to kidney transplants! However, before we move on, I should admit that I have been oversimplifying things somewhat. Though the Hungarian Maximum Matching Algorithm will work on a huge variety of graphs, there are some graphs that will trip it up. For example, consider the graph and matching below, taken from this stack exchange post by Misha Lavrov.\n\nTrying to find an augmenting path from vertex 1, the algorithm will go from 1 to 2 to 8 to 9 to 10, before getting stuck and concluding that there is no augmenting path, and thus that the given matching is a maximum matching. Similarly, if it starts from vertex 4 it will follow the sequence of edges 4–3, 3–7, 7–6, 6–5 before it again gets stuck and terminates.\nGraphs that run into these kinds of problems are always non-bipartite, meaning they contain cycles of odd numbers of edges, like the pentagons in the graph above. To ensure that, if they exist, augmenting paths can always be found in these types of graph, the Hungarian Maximum Matching Algorithm needs to be modified. The first person to find a way to do this was computer scientist Jack Edmonds, who introduced an ingenious method for adapting the algorithm to deal with odd length cycles in his ‘blossom’ algorithm in 19655. The details of his algorithm are fascinating, but beyond the scope of this article, which is why I have only used bipartite graphs as examples, and will continue to do so in the next section. If you want to learn how a maximum matching can be found for any graph using Edmonds’ blossom algorithm, I recommend this excellent youtube video by Tomáš Sláma."
  },
  {
    "objectID": "posts/kidney-transplants-and-graph-theory/index.html#how-matching-algorithms-apply-to-kidney-transplants",
    "href": "posts/kidney-transplants-and-graph-theory/index.html#how-matching-algorithms-apply-to-kidney-transplants",
    "title": "Kidney Transplants and Graph Theory",
    "section": "How Matching Algorithms Apply to Kidney Transplants",
    "text": "How Matching Algorithms Apply to Kidney Transplants\nPeople with end-stage renal disease have kidneys that are unable to perform their function of filtering the blood and are, therefore, unable to survive without undergoing regular dialysis unless they receive a working kidney transplant from a donor. Many people with this condition will have family member or loved one who is willing to donate a kidney to them – as most of us only need one working kidney to live a healthy life – but this is often not possible due to a mismatch between the donor and the intended recipient’s blood and/or antigen type. For example, imagine that a patient with advanced chronic kidney disease, who we’ll call Recipient A, has a sibling, Donor A, who is willing to donate a kidney to them but cannot because they have incompatible blood groups. In the past, this would have meant that Donor A would have been unable to donate a kidney in order to help their sibling6. Nowadays, however, the pair have another option; they can participate in a the UK Living Kidney Sharing Scheme7. Suppose another recipient and donor pair, a parent and child, say, are in the same situation. If the donor in this pair – let’s call them Donor F for reasons that will shortly become clear – is a match with Recipient A, and Donor A’s kidney is compatible with the recipient, Recipient F, then the two pairs can effectively ‘swap’ kidneys, in what is called a paired exchange.\n\nWe can represent possible paired kidney exchanges between pairs of donors and recipients in a graph, with each vertex representing one incompatible pair, and an edge linking two pairs signifying that a paired exchange is possible between them. Consider the scenario described above; this would be represented by a graph with two vertices and one edge, A–B.\n\nWhat happens if we introduce more incompatible pairs of donors and recipients? Well, we might end up with a graph like \\(S\\), but this time with the vertices representing patients in need of kidneys and loved ones who are unable to denote to them. In reality a graph of incompatible donor-recipient pairs in a country like the UK would have hundreds of vertices, but we can use a smaller one to help us think about the problem of matching compatible pairs.\n\nAs we saw earlier, several different matchings of this graph exist, but some of them will result in more people receiving kidneys than others. For example, if the Pair A’s transplant centre search a regional or national database for a pair that Pair can exchange a kidney with and stop at the first one they find, then Pair A might be matched with Pair D. This would leave Pair B and Pair C free to participate in a paired exchange, but Recipients E and F would then be left without a kidney. If a maximum matching algorithm is used on the other hand, all six recipients will be matched with a compatible donor.\n\nIn 2004, a team from John Hopkins Hospital in the US proposed using Edmonds’ blossom algorithm to optimize kidney exchanges8. They used simulated pools of donor-recipient pairs to test their scheme, and found that it would result in more transplants, with better antigen matches, than the ‘first-accept’ system that was in use at the time9. Of course, employing an algorithm to decide who gets which organs presents numerous ethical and logistical challenges. For instance, what happens when there is more than one possible maximum matching, resulting in different people getting kidney transplants? Consider what our example graph would look like if we discovered that Recipient B was actually not compatible with Donor E.\n\nThis graph has no perfect matching – one in which all vertices are matched – but it does have two different maximum matchings; the matching A–D, B–C and the matching A–F, C–D.\nWhich kidney transplants should be performed? Should Recipients E and F or Recipients B and E have to remain on dialysis longer as they wait for the next run of the matching algorithm to be performed, or for a suitable deceased donor to match with them? Should the amount of time the recipients have spent on the waiting list for a kidney factor into this decision? What about their ages or levels of frailty? These are difficult questions that no computer can answer for us.\nWe should also note that compatibility between donors and recipients is not actually as binary as ‘a match’ or ‘not a match’. There are six antigens which are particularly important in organ transplantation10 and it is rare that donors and recipients will have the same type of each of them, but more antigens in common are still better than fewer11. The difference between a donor and recipients age, and whether or the recipient has been sensitised to certain antigens by pregnancy or a previous transplant or blood transfusion, can also influence the chances of success.\nDespite these important concerns, Edmonds’ blossom algorithm is an integral part of the process that NHS Blood and Transplant uses to find possible kidney paired exchanges in the UK. However, it is not the only method used, but is instead embedded as a step within a larger algorithm, developed in conjunction with computer scientists at the University of Glasgow12. As well as finding the largest possible matching, their procedure seeks to maximise the total ‘weight’ of a matching, where the weights given to edges of the graph are calculated by a scoring system used by NHSBT13 that takes into account:\n\nThe number of previous matching runs the recipients have participated in (matching runs take place every three months).\nHow sensitised the recipients are to foreign antigens.\nHow well matched donor and recipients’ antigens are.\nThe age difference between donors and recipients.\nHow much travel would be required for the participants in a transplant.\n\nAnother way in which the algorithm used by NHS Blood and Transport differs from a straightforward implementation of Edmond’s Blossom algorithm is that it allows for non-directed altruistic donations and 3-way exchanges. If you would like to understand how this is done, I recommend the article written by Dr David Manlove of the University of Glasgow on his and his colleagues work on the algorithm used by the UK Living Kidney Sharing Scheme in edition 475 of the London Mathematical Society Newsletter."
  },
  {
    "objectID": "posts/kidney-transplants-and-graph-theory/index.html#further-reading",
    "href": "posts/kidney-transplants-and-graph-theory/index.html#further-reading",
    "title": "Kidney Transplants and Graph Theory",
    "section": "Further Reading",
    "text": "Further Reading\n\nPrimum non nocere (First, do no harm) – an article by Maria Ibrahim about the role of statistics and statistical analysis in transplant medicine\nOne day. Six operations. Three kidneys. The story of an organ donor chain – an article by Rachel Williams about a 3-way exchange"
  },
  {
    "objectID": "posts/kidney-transplants-and-graph-theory/index.html#footnotes",
    "href": "posts/kidney-transplants-and-graph-theory/index.html#footnotes",
    "title": "Kidney Transplants and Graph Theory",
    "section": "References",
    "text": "References\n\n\n“Annual Activity Report,” NHS Blood and Transplant, Accessed February 16, 2025.↩︎\n“How living organ donors change lives,” NHS Blood and Transplant, Accessed February 16, 2025.↩︎\nEric Weisstein, “Matching,” Wolfram Mathworld, August 3, 2016.↩︎\n“Kuhn’s Algorithm for Maximum Bipartite Matching,” Algorithms for Competitive Programming, July 17, 2023.↩︎\nRichard L. Apodaca, “The Maximum Matching Problem,” Depth-First, April 3, 2019.↩︎\nDavid Manlove, “How Operational Research Helps Kidney Patients in the UK,” Impact 2018, no. 1 (2018): 16–19.↩︎\n“Who can be a living donor?,” NHS Blood and Transplant, Accessed February 16, 2025.↩︎\nHari Jagannathan Balasubramanian, “The mathematics of matching kidneys,” Thirty letters in my name, March 28, 2009.↩︎\nDorry Segev et al., “Kidney Paired Donation and Optimizing the Use of Live Donor Organs,” JAMA 293, no. 15 (April 2004): 1883–1890.↩︎\n“Matching and Compatibility,” UC Davis Health, Accessed February 16, 2025.↩︎\n“Tissue typing for kidney donation,” Great Ormond Street Hospital, May, 2015.↩︎\nDavid Manlove and Gregg O’Malley, “Paired and Altruistic Kidney Donation in the UK: Algorithms and Experimentation,” Journal of Experimental Algorithmics 19 (2014): 1–21↩︎\n“Living Donor Kidney Matching Run Process,” NHS Blood and Transplant, Accessed September 2, 2024.↩︎"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis article was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nIf I am screened for a disease and receive a positive test result, how worried should I be? What are the chances it was a false positive? In this article I’ll discuss why medical test results aren’t always as straightforward as they might seem, and how an equation first discovered over 200 years ago can help us to understand them better."
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#sensitivity-and-specificity",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#sensitivity-and-specificity",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\nWhen interpreting a diagnostic test result, two of most important pieces of information we need to know are the sensitivity and specificity of the test. Coined in 1947 by Jacob Yerushalmy1, these terms refer respectively to the probability that a diagnostic test will correctly identify someone with a disease, and the probability that it will accurately distinguish those without the condition.\nSteven McGee gives a helpful illustration of these concepts in his book Evidence-Based Physical Diagnosis2, using a hypothetical experiment. He asks us to imagine that a specific physical examination finding – a holosystolic heart murmur – is assessed in a group of 42 people with a particular type of valvular heart disease and a group of 58 people without the condition, producing the following results.\n\n  \n\nPhysical Examination Findings in Patients With and Without Tricuspid Regurgitation\n\n\n\nValvular Heart Disease Status\n\n\nPatients With Tricuspid Regurgitation\nPatients Without Tricuspid Regurgitation\n\n\nPhysical Examination Finding\nHolosystolic Murmur Present\n22\n3\n\n\nHolosystolic Murmur Absent\n20\n55\n\n\n\n\n\nThe sensitivity of a test for a disease is the proportion of patients who have the condition and test positive. To calculate it we can divide the number of true positives by the sum of the number of true positives and the number of false negatives.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(\\text{FP}\\) denotes the number of false positives.\n\\(\\text{FN}\\) denotes the number of false negatives.\n\\(\\text{TP}\\) denotes the number of true positives.\n\\(\\text{TN}\\) denotes the number of true negatives.\n\n\n\nFor the example above this gives us a specificity of\n\\[\n\\text{Sensitivity}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}=\\frac{22}{22+20}=\\frac{22}{42}\\approx0.52,\n\\] equivalent to around 52%. We can find the specificity – the proportion of patients without the disease who correctly test negative – similarly;\n\\[\n\\text{Specificity}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}=\\frac{22}{55+3}=\\frac{55}{58}\\approx0.95.\n\\]"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#the-base-rate-fallacy",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#the-base-rate-fallacy",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "The Base Rate Fallacy",
    "text": "The Base Rate Fallacy\nNow we’re clear on the definitions of sensitivity and specificity, let’s look at a common way in which these values can be misinterpreted. Imagine there is a medical condition, let’s call it Bayesitis, with a prevalence of 5% – meaning it affects one in 20 people. Now suppose that a blood test for Bayesitis is found to have a sensitivity of 99% and a specificity of 85%. If a random member of the population has their blood tested and receives a positive result, what is the probability that it is a false positive and they don’t actually have the disease?\nWe might intuitively assume that the answer is 15% since, according to the definition of specificity, this is the proportion of patients without Bayesitis who will test positive incorrectly. Unfortunately, by making this assumption, we are falling into the trap of the base rate fallacy.\nLet’s see if we can get a ballpark figure for the true probability by thinking through what would happen if we were to screen a representative sample of 20 people for Bayesitis. We’ll assume that one person in our sample – 5% – has the disease. Since the sensitivity of our blood test is 99% this person is very likely to test positive, giving us one true positive. Now, what about the 19 people in our sample who do not have the disease? Referring to the specificity of our blood test, we can expect it to correctly identify 85% percent of them, giving us 16.15 true negatives. Since we can’t have 0.15 of a person, let’s round this to 16, which leaves us with three false positives.\n\nSo three out of four positive test results from our representative sample are false, meaning that if someone from our sample receives a positive test result, the probability that they don’t actually have Bayesitis is 75%. This is far enough from our initial guess of 15% for us to know that we must have made an error in our reasoning.\nOur problem was that we failed to take into account the population prevalence, or base rate, of Bayesitis. To see the importance of the base rate, consider a group of people in which 0% have the condition – in a population like this, there is a 100% probability that any positive test results are false.\nClearly, the sensitivity and specificity of a test aren’t on their own sufficient for us to calculate the probability that any particular positive test result is false. How, then, do we find the exact value of this probability?"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#bayes-to-the-rescue",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#bayes-to-the-rescue",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Bayes to the Rescue",
    "text": "Bayes to the Rescue\nThe answer, of course, is Bayes’ Theorem. Discovered by Thomas Bayes in the 1740s and given its modern form by Pierre Simon Laplace in the 1810s3, this famous theorem states that for any two events \\(A\\) and \\(B\\), we can get the conditional probability of \\(A\\) given \\(B\\) from the conditional probability of \\(B\\) given \\(A\\) using the formula\n\\[\n\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)},\n\\] where \\(\\mathbb{P}(A)\\) and \\(\\mathbb{P}(B)\\) are the marginal (unconditional) probabilities of \\(A\\) and \\(B\\) respectively.\nIn our example, \\(A\\) corresponds to the event that the person who had their blood tested actually has Bayesitis, and \\(B\\) corresponds to the event that they test positive for the disease. When we committed the base rate fallacy, we made the mistake of assuming that \\(\\mathbb{P}(A|B)\\) was equivalent to \\(\\mathbb{P}(B|A)\\), and we failed to consider the marginal probabilities \\(\\mathbb{P}(A)\\) and \\(\\mathbb{P}(B)\\). Let’s try to work out the probability that their test result was a false positive again, this time using Bayes’ theorem.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(D^{-}\\) denotes not having the disease.\n\\(D^{+}\\) denotes having the disease.\n\\(T^{-}\\) denotes testing negative.\n\\(T^{+}\\) denotes testing positive\n\n\n\nWe want to find\n\\[\n\\mathbb{P}(D^{-}|T^{+})=\\frac{\\mathbb{P}(T^{+}|D^{-})\\mathbb{P}(D^{-})}{\\mathbb{P}(T^{+})},\n\\]\nWe know that \\(\\mathbb{P}(D^{-})=0.95\\), equivalent to 95%, since the prevalence of Bayesitis in the population is 95%. We also know \\(\\mathbb{P}(T^{+}|D^{-})=0.15\\), as we defined our imaginary test to be 85% accurate. So to find the probability of a positive test result being false we just need to find the denominator of the fraction above; \\(\\mathbb{P}(T^{+})\\), the overall probability of testing positive.\nWe can write\n\\[\n\\begin{split}\n\\mathbb{P}(T^{+})&=\\mathbb{P}(T^{+}\\cap(D^{+}\\cup D^{-}))\n\\\\&=\\mathbb{P}(T^{+}\\cap D^{+})+\\mathbb{P}(T^{+}\\cap D^{-})\n\\\\&=\\mathbb{P}(T^{+}|D^{+})\\mathbb{P}(D^{+})+\\mathbb{P}(T^{+}|D^{-})\\mathbb{P}(D^{-}),\n\\end{split}\n\\] using the fact that \\(D^{+}\\) and \\(D^{-}\\) are collectively exhaustive and mutually exclusive events, as well as the definition of conditional probability, which we can rearrange to get that \\(\\mathbb{P}(A\\cap B)=\\mathbb{P}(A|B)\\mathbb{P}(B)\\).\nNow, to work out \\(\\mathbb{P}(T^{+})\\), we can use our knowledge of the sensitivity and specificity of our blood test and the prevalence of Bayesitis to subsitute in the values \\(\\mathbb{P}(T^{+}|D^{+})=0.99\\), \\(\\mathbb{P}(D)=0.05\\), \\(\\mathbb{P}(T^{+}|D^{-})=0.15\\), and \\(\\mathbb{P}(D^{-})=0.95\\), finding that\n\\[\n\\mathbb{P}(T^{+})=0.99\\times 0.05 + 0.15\\times 0.95 = 0.192.\n\\] Finally, we can calculate that\n\\[\n\\mathbb{P}(D^{-}|T^{+})=\\frac{0.15\\times0.95}{0.192}=0.7421875,\n\\] which is pretty close to the estimate we got from our thought experiment involving 20 people!\nTo check our answer, we can use R to simulate repeatedly sampling 20 people from a population of 1 million people with a 5% prevalence of Bayesitis.\n\n\nSimulating Bayesitis Screening in R\n# Because we will be using randomly generated numbers, we first set a seed to ensure our results are reproducible:\nset.seed(31415926) \n# We now set up some variables:\npop_size &lt;- 1000000\nbayesitis_prevalence &lt;- 0.05\nbayesitis_pop_size &lt;- pop_size * bayesitis_prevalence # we calculate 5% of the population to find the number of people that have Bayesitis\n# Now imagine that each person in the population is assigned a number randomly pick 5% of these numbers to designate the people that have Bayesitis\ntotal_pop &lt;- 1:pop_size # we create a vector made up of the numbers from 1 to 1,000,000\nbayesitis_pop &lt;- sample(total_pop, size=bayesitis_pop_size, replace=FALSE) # we randomly samples 5% of the entries from our total_pop vector\n# Let's create a logical vector (one that only contains TRUE and FALSE values) to represent the Bayesitis status of every member of the population:\nbayesitis_statuses &lt;- rep(FALSE, times=pop_size) # we initialise a vector the length of the population where every entry is FALSE\nbayesitis_statuses[bayesitis_pop] &lt;- TRUE # we change the entries corresponding to the people with Byesitis to TRUE\n# We can now set up our simulations: \nnum_sims &lt;- 300\nfalse_pos_props &lt;- rep(NA, times=num_sims) # we initialise a vector to store the proportion of false positives we get from each simulation \n# Now let's repeatedly pick 20 random people from the population and simulate screening them for Bayesitis:\nsamp_size &lt;- 20\nfor (n in 1:num_sims) {\n  rand_samp &lt;- sample(total_pop, size=samp_size, replace=FALSE)\n  # Let's set up variables to store the number of true and false positive test results:\n  num_true_pos &lt;- 0\n  num_false_pos &lt;- 0\n  rand_nums &lt;- runif(samp_size, min=0, max=1) # we generate 20 random numbers between 0 and 1, so that everyone in the sample gets a correct/incorrect\n                                              # test result with a probability according to the sensitivity and specificty of our test\n  for (i in 1:samp_size) {\n    true_status &lt;- bayesitis_statuses[rand_samp[i]]\n    if (rand_nums[i] &lt;= 0.99 & true_status == TRUE) { # if the person does have Bayesitis they get an accurate positive result with a probability of 99%\n      num_true_pos &lt;- num_true_pos + 1\n    } else if (rand_nums[i] &gt; 0.85 & true_status == FALSE) { # if they don't they get a false positive result with a probability of 15%\n      num_false_pos &lt;- num_false_pos + 1\n    }\n  }\n  total_pos &lt;- num_true_pos + num_false_pos\n  if (total_pos != 0) { # if there are no positive test results, true or false, the proportion of positive test results that are false is undefined\n    false_pos_props[n] &lt;- num_false_pos / total_pos\n  }  \n}\n# Now let's see how the average proportion of positive test results that are false changes as the number of simulations we've done increases:\nprops_for_avg &lt;- false_pos_props[!is.na(false_pos_props)] # we remove the undefined values from our vector of empirical false positive probabilities\ncum_avg &lt;- cumsum(props_for_avg) / seq_along(props_for_avg) # we calculate a cumulative average as the number of simulations increases\n# We plot the cumulative average against the number of simulations:\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\")\nplot(cum_avg, ylim=c(0.5,1), xlab=\"Number of Simulations\", ylab=\"Average False Positive Proportion\", type=\"l\", col=\"#808080\", lwd=4.2, cex.lab=1.5, cex.axis=1.5)\nabline(h=0.7421875, col=\"#009ed8\", lty=1, lwd=4.2)\nlegend(\"topright\", legend=\"Calculated Probability\", col=\"#009ed8\", lty=1, lwd=4.2, cex=1.2)\n\n\n\n\n\n\n\n\n\nReassuringly, as we do more simulations, the average proportion of positive test results that are false appears to converge to our calculated value of \\(\\mathbb{P}(D^{-}|T^{+})\\)."
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#further-reading",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#further-reading",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Further Reading",
    "text": "Further Reading\n\nBayes theorem and likelihood ratios for diagnostic tests – a blog post by Stathis Kamperis about likelihood ratios\nAn Interactive Fagan Nomogram – a blog post by Carlos Scheidegger about the Fagan nomogram"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#footnotes",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#footnotes",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "References",
    "text": "References\n\n\nJacob Yerushalmy, “Statistical problems in assessing methods of medical diagnosis with special reference to x-ray techniques,” Public Health Reports 62, no. 2 (October 1947): 1432–1439.↩︎\nSteven McGee, Evidence-Based Physical Diagnosis. 3rd ed. Philadelphia: Saunders, 2012.↩︎\nSharon Bertsch McGrayne, The Theory That Would Not Die. New Haven: Yale University Press, 2011.↩︎"
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html",
    "href": "posts/observational-studies-and-causal-inference/index.html",
    "title": "Observational Studies and Causal Inference",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis blog post was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nRandomised Controlled Trials (RCTs) are widely recognised as the gold standard of study designs, but they are not always possible to perform. For example, it would have been neither ethical nor feasible for Richard Doll and Austin Bradford Hill to randomly assign a group of volunteers to a smoking and non-smoking group in order to show that cigarettes cause lung cancer. However, they were still able to establish a link between smoking and the disease, using first a case-control study and then a prospective cohort study1.\nThese are both types of observational study2, an alternative to RCTs that can be very useful – as in the case of smoking and lung cancer. Unfortunately, the fact the participants are not randomised to different groups in observational studies can make their results particularly difficult to interpret. In this article, I will discuss how diagrams representing causal relationships can help us to spot potential sources of bias in these kinds of studies."
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#causal-inference",
    "href": "posts/observational-studies-and-causal-inference/index.html#causal-inference",
    "title": "Observational Studies and Causal Inference",
    "section": "Causal Inference",
    "text": "Causal Inference\nCausal relationships are notoriously difficult to distinguish from mere statistical associations, as summed up in the famous saying “correlation does not imply causation”. A humorous example of this can be found in an educational statistics paper in which the author investigates the fairy tale that storks deliver babies by analysing 1980s estimates of stork populations and birth rates in seventeen different European countries3.\n\n  \n\nEstimates of European Stork Populations and Birth Rates in the 1980s\n\n\n\nCountry\n\n\nAlbania\nAustria\nBelgium\nBulgaria\nDenmark\nFrance\nGermany\nGreece\nHolland\nHungary\nItaly\nPoland\nPortugal\nRomania\nSpain\nSwitzerland\nTurkey\n\n\nStork Pairs\n100\n300\n1\n5000\n9\n140\n3300\n2500\n4\n5000\n5\n30,000\n1500\n5000\n8000\n150\n25,000\n\n\nYearly Births\n83,000\n87,000\n118,000\n117,000\n59,000\n774,000\n901,000\n106,000\n188,000\n124,000\n551,000\n610,000\n120,000\n367,000\n439,000\n82,000\n1,576,000\n\n\n\n\n\nWe can plot these data and observe a linear relationship between stork populations and birth rates, with a correlation coefficient of 0.62 to two significant figures.\n\n\nPlotting the Data and Calculating the Correlation Coefficient in R\nstork_pairs &lt;- c(100, 300, 1, 5000, 9, 140, 3300, 2500, 4, 5000, 5, 30000, 1500, 5000, 8000, 150, 25000)\nyearly_births &lt;- c(83000, 87000, 118000, 117000, 59000, 774000, 901000, 106000, 188000, 124000, 551000, 610000, 120000, 367000, 439000, 82000, 1576000)\ncorrelation &lt;- cor(stork_pairs, yearly_births) # calculating the pearson correlation coefficient\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nplot(stork_pairs, yearly_births, # plotting the data points\n     xlab=\"Stork Pairs\", ylab=\"Yearly Births\", \n     pch=4, cex=1.8, col=\"#009ed8\", lwd=4.8, cex.lab=1.5, cex.axis=1.5)\nabline(lm(yearly_births ~ stork_pairs), # adding the line of best fit\n       col = \"#ffc000\", lty=2, lwd = 4.2)\nlegend(\"topleft\", legend=paste0(\"Line of Best Fit  (r=\", round(correlation, 2), \")\"), \n       col=\"#ffc000\", lty=2, lwd=4.2, cex=1.5)\n\n\n\n\n\n\n\n\n\nMoreover, performing a significance test for correlation with the null hypothesis that there is no relationship between stork populations and birth rates gives us a p-value of less than 0.05, a common threshold for a result to be considered statistically significant4.\n\n\nTesting the Null Hypothesis in R\nsignificance_test &lt;- cor.test(stork_pairs, yearly_births)\np_val &lt;- significance_test$p.value\ncat(\"p-value:\", p_val)\n\n\np-value: 0.007897861\n\n\nClearly, more storks do not cause more babies to be born. But the non-causality of statistical associations might not always be so obvious. To deal with the challenges of understanding causes and effects, a new branch of statistics known as ‘causal inference’ has emerged over the past few decades5.\nThis type of statistics often involves the use of directed acyclic graphs – ‘DAGs’ for short – to model the relationships between variables. Let’s explore the essential features of these diagrams.\n\nDirected Acyclic Graphs\nIn discrete mathematics, a graph is a structure made up of vertices linked by edges. A path between two vertices is a sequence of edges linking them together6.\n\nFor example, in the above graph, there is a path between A and C along the the sequence of edges between vertices A, B, D, E, and C.\nDirected graphs are a particular type of graph in which all the edges are arrows. In these types of graph a vertex can be an descendant or ancestor of another vertex if there is a path of arrows all pointing in the same direction between them; the descendant lies at the arrowhead and ancestors lies at the tail. If this path is only one edge long, the ancestor can be called the descendant’s parent and the descendant can be called the parent’s child7.\n\nHere, B is a parent (and thus also an ancestor) of C and D, a child (and descendant) of A, and and ancestor of E.\nDirected graphs are ideal for causal inference, because we can use vertices to represent the variables and edges to represent the causal relationships between them, with parents having a direct effect on their children. For example, a simplified DAG showing the effects of different factors on a person’s chance of developing lung cancer might show the variables ‘Genetics’, ‘Environmental Factors’, and ‘Smoking’ all to be parents of the variable ‘Lung Cancer Risk’.\n\nAn acyclic graph is simply one that contains no cycles (a cycle is a directed path leading from a variable back to itself), so when we use a DAG to model causal relationships a variable cannot cause itself.\nAn example of a DAG which might explain the correlation between the stork populations and birth rates of European countries could introduce extra variables, such as land area and human population size.\n\nThis DAG could account for the association between the ‘Stork Population’ and ‘Birth Rate’ variables without requiring there to be a causal relationship between them; their correlation can be explained by the fact that ‘Land Area’ is both a parent of ‘Stork Population’ and an ancestor of ‘Birth Rate’.\nThere are several structures that commonly appear appear in DAGs representing observational studies. In the rest of this article I want to focus on two of the most important – confounders and colliders – and explore how both of them can help to explain misleading results in medical research."
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#confounders",
    "href": "posts/observational-studies-and-causal-inference/index.html#confounders",
    "title": "Observational Studies and Causal Inference",
    "section": "Confounders",
    "text": "Confounders\nWhen we are investigating the effect of a variable X on another variable Y using an observational study, one of the most common ways we can be misled is by confounding8. This occurs when a third variable has an effect on both X and Y, which can lead to them being correlated even if there is not in fact a causal relationship between them. For instance, we could say that land area is a confounder when we are investigating the relationship between stork populations and birth rates, because an increase in land area leads to an increase in both variables. In a DAG, we can define a confounder to be a variable on a path between X and Y which is an ancestor of both of them9.\n\nIn the most extreme cases, confounding can actually mean that the true effect of X on Y is reversed, in a phenomenon known as Simpson’s paradox10. Robert Heaton gives an helpful illustration of this in his blog post ‘Making peace with Simpson’s Paradox’. He asks us to imagine a hypothetical illness with a specific drug treatment that causes faster recovery in higher dosages. This would seem quite straightforward to prove by collecting data on the amount of treatment different patients were given and the time it took them to recover. Then, however, he complicates the scenario by introducing a third variable – age. If older people tend to recover more slowly, and also tend to require higher dosages of the treatment drug to recover at all, an observational study that does not control for age may actually end up showing a negative correlation between drug dosage and recovery speed.\n\n\nSimulating and Plotting Data to Illustrate Robert Heaton’s Example of Simpson’s Paradox in R\nset.seed(31415) \ngroup_1_dosages &lt;- sample(1:20, 20, replace = TRUE)\ngroup_2_dosages &lt;- sample(21:40, 20, replace = TRUE)\ngroup_3_dosages &lt;- sample(41:60, 20, replace = TRUE)\nall_dosages &lt;- c(group_1_dosages, group_2_dosages, group_3_dosages)\ngroup_1_recovery_speeds &lt;- 1.2 * group_1_dosages + 100 + rnorm(20,0,5)\ngroup_2_recovery_speeds &lt;- 1.2 * group_2_dosages + 50 + rnorm(20,0,5)\ngroup_3_recovery_speeds &lt;- 1.2 * group_3_dosages + rnorm(20,0,5)\nall_recovery_speeds &lt;- c(group_1_recovery_speeds, group_2_recovery_speeds, group_3_recovery_speeds)\npar(mar=c(bottom=2.4, left=3, top=2.4, right=0.9), family=\"Roboto Slab\")\nplot(group_1_dosages, group_1_recovery_speeds, xlim=c(-3,72), ylim=c(36,120), bty=\"n\", axes = FALSE, pch=16, cex=2.4,  col=\"#009ed8\") \npoints(group_2_dosages, group_2_recovery_speeds, pch=17, cex=2.4,  col=\"#009ed8\")\npoints(group_3_dosages, group_3_recovery_speeds, pch=18, cex=2.4,  col=\"#009ed8\")\narrows(x0=0, y0=39, x1=72, y1=39, length=0.15, code=2, lwd=3, col=\"#808080\") \narrows(x0=0, y0=39, x1=0, y1=120, length=0.15, code=2, lwd=3, col=\"#808080\") \nmtext(\"Drug Dosage\", side=1, line=1, cex=1.8)\nmtext(\"Recovery Speed\", side=2, line=0, cex=1.8)\nabline(lm(group_1_recovery_speeds ~ group_1_dosages), col=\"#006a90\", lwd=4.8, lty=2) \nabline(lm(group_2_recovery_speeds ~ group_2_dosages), col=\"#006a90\", lwd=4.8, lty=2) \nabline(lm(group_3_recovery_speeds ~ group_3_dosages), col=\"#006a90\", lwd=4.8, lty=2) \nabline(lm(all_recovery_speeds ~ all_dosages), col=\"#ffc000\", lwd=4.8, lty=2)\nlegend(x=54, y=120, legend=c(\"20–39 Year Olds \", \"40–59 Year Olds \", \"60–79 Year Olds \"), pch=c(16, 17, 18), col=\"#009ed8\", cex=1.5)\nlegend(x=3, y=54, legend=c(\"Overall Line of Best Fit \", \"Age Group Lines of Best Fit \"), lty=2, col=c(\"#ffc000\",\"#006a90\"), lwd=4.2, cex=1.5)\n\n\n\n\n\n\n\n\n\nIn the simulated data plotted above, a higher dosage of the treatment drug corresponds with a faster recovery speed within each age category. However, when we combine data from all the age categories we find a negative correlation between drug dosage and recovery speed.\n\n\nFinding the Correlation Coefficient in R\ncorrelation &lt;- cor(all_dosages, all_recovery_speeds)\ncat(\"Pearson Correlation Coefficient:\", correlation)\n\n\nPearson Correlation Coefficient: -0.7453944\n\n\nWhen we draw the DAG for this example, we can see that the variable ‘Age’ is acting as a counfounder, obscuring the true causal relationship between the ‘Drug Dosage’ and ‘Recovery Speed’ variables.\n\nSimpson’s paradox has been observed several times in the medical literature, perhaps most famously in a 1986 observational study by a group of urologists on the effectiveness of different types of procedure for the removal of kidney stones11. Wisely, the researchers recognised that stone size might be a confounding factor, so they split their data according to whether the original stone was less than two centimetres in diameter or not.\n\n  \n\nObserved Success Rate of Kidney Stone Removal Procedures\n\n\n\nSuccess Rate\n\n\nOpen Surgery\nPercutaneous Nephrolithotomy\n\n\nStone Size\nSmall\n≈ 93% \\(\\left( \\frac{81}{87} \\right)\\)\n≈ 87% \\(\\left( \\frac{234}{270} \\right)\\)\n\n\nLarge\n≈ 73% \\(\\left( \\frac{192}{263} \\right)\\)\n≈ 69% \\(\\left( \\frac{55}{80} \\right)\\)\n\n\nBoth\n≈ 78% \\(\\left( \\frac{273}{350} \\right)\\)\n≈ 83% \\(\\left( \\frac{289}{350} \\right)\\)\n\n\n\n\n\nThe urologists found that open surgery had been more successful than percutaneous nephrolithotomy in both small and large stones. But if they had not stratified their data by stone size, they would have obtained a higher overall success rate for percutaneous nephrolithotomy than for open surgery. The reason for this was that patients with small stones were more likely to be treated with percutaneous nephrolithotomy than patients with larger stones, and these were also the patients most likely to have a good result regardless of the procedure they underwent. So stone size influenced both the choice of treatment and the treatment outcome, and thus acted as a counfounder.\n\nThe size of a patient’s stone might have influenced their probability of having one treatment over another for any number of reasons. For instance, perhaps patients with smaller stones were more inclined to be concerned about the side effects of open surgery, or perhaps there was a longer waiting list for percutaneous nephrolithotomy that patients with larger stones tend to be less willing to tolerate. (These are just examples of possible explanations, not necessarily the true reasons that the size of kidney stones affected doctors and patients’ choice of treatment in the 1986 study.)"
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#colliders",
    "href": "posts/observational-studies-and-causal-inference/index.html#colliders",
    "title": "Observational Studies and Causal Inference",
    "section": "Colliders",
    "text": "Colliders\nAnother issue that can arise in observational studies is collider bias. When we are investigating the effect of X on Y, a collider is a variable lying on a path between X and Y where two causal arrows meet, meaning the collider has more than one parent.\n In observational medical studies, it is not uncommon that the variables we are investigating, X and Y, both have a causal effect on the probability that a patient is included in the study. In these cases, selection into the study is itself a collider variable.\nTo understand how collider bias can cause unrelated variables to appear to have a correlation, let’s imagine a hypothetical virus called ‘colliditis’. We’ll supose that the severity of disease experienced by people who catch colliditis is unrelated to the number of cigarettes they smoke in a day. For simplicity, we can assume that the average number of cigarettes of smoked a day and the severity of disease are uniformly distributed in the population of people who have caught colliditis. So if we plotted a random sample of one thousand people from the population of people with the viral infection, it would look like this.\n\n\nPlotting the Relationship Between Disease Severity and Average Number of Cigarettes Smoked in a Day in a Simulated Sample of People Infected With Colliditis in R\nset.seed(31415)\nn &lt;- 1000 \ninfected_sample &lt;- matrix(0, nrow=n, ncol=2) # we initialise an empty matrix to store the data for each person\nfor (i in 1:n) {\n  severity &lt;- runif(1, min=0, max=20) # we assign each person in the sample a random severity score between 0 and 20\n  cigarettes &lt;- runif(1,0,20) # we assign each person in the sample a random average number of cigarettes smoked per day between 0 and 20\n  infected_sample[i,1] &lt;- severity\n  infected_sample[i,2] &lt;- cigarettes\n}\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") \nplot(infected_sample[,1], infected_sample[,2], pch=4, cex=1.2, lwd=2.4, col=\"#808080\",\n     xlab=\"Colliditis Severity Score\", ylab=\"Average Number of Cigarettes Smoked in a Day\",\n     cex.lab=1.5, cex.axis=1.5)\nabline(lm(infected_sample[,1] ~ infected_sample[,2]), lwd=4.8, col=\"#009ed8\",) # we plot the line of best fit\n\n\n\n\n\n\n\n\n\nNow suppose that out of this sample, 90% the people with a colliditis severity score ≥15 are in hospital, and of those with a colliditis severity score &lt;15, those who smoke ≥10 cigarettes a day on average have a 60% chance of being in hospital, compared to a 10% chance for those who smoke &lt;10 cigarettes a day on average. If we plot cigarettes vs colliditis severity of only those in our sample who are hospitalised, we see an interesting result.\n\n\nPlotting the Relationship Between Disease Severity and Average Number of Cigarettes Smoked in a Day in a Simulated Sample of Hospitalised People Infected With Colliditis in R\nset.seed(31415)\nhospital_infected_sample &lt;- matrix(rep(NA, 2*n), nrow=n, ncol=2) \nfor (i in 1:n) {\n  dice_roll &lt;- runif(1, min=0, max=1) # we generate random number between 0 and 1\n  person_i &lt;- infected_sample[i,]\n  if(person_i[1] &gt;= 15 && dice_roll &lt;= 0.9) { # these people have a 90% chance of being in hospital\n    hospital_infected_sample[i,] &lt;- person_i\n  } else if(person_i[2] &gt;= 10 && dice_roll &lt;= 0.6) { # these people have a 60% chance of being in hospital\n    hospital_infected_sample[i,] &lt;- person_i\n  } else if(person_i[2] &lt; 10 && dice_roll &lt;= 0.1) { # others have a 10% chance of being in hospital\n    hospital_infected_sample[i,] &lt;- person_i\n  }\n}\n# Now we remove the people not in hospital from our sample:\nhospital_infected_sample &lt;- na.omit(hospital_infected_sample)\n# And plot the result:\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") \nplot(hospital_infected_sample[,1], hospital_infected_sample[,2], pch=4, cex=1.2, lwd=2.4, col=\"#808080\",\n     xlab=\"Colliditis Severity Score\", ylab=\"Average Number of Cigarettes Smoked in a Day\",\n     cex.lab=1.5, cex.axis=1.5)\nabline(lm(hospital_infected_sample[,1] ~ hospital_infected_sample[,2]), lwd=4.8, col=\"#009ed8\",) # we plot the line of best fit\n\n\n\n\n\n\n\n\n\nIf we took the hospitalised population as representative of the general population we would find that there is a negative relationship between average number of cigarettes smoked in a day and severity of colliditis. But we would be failing to account for the fact that having a severe case of colliditis, and smoking a large number of cigarettes per day are both factors that make you more likely to be in hospital. So if you are in hospital with a low colliditis score, you are more likely to smoke a lot of cigarettes than if you are in hospital with a high colliditis score, since there must be some other reason for your hospitalisation than colliditis, and this reason may be smoking.\nThe DAG for this example would show that ‘Hospitalisation’ is a collider, and that there is no true causal relationship between the variables ‘Smoking’ and ‘Severity’.\n\nFor an example of collider bias in the medical literature, let’s turn to a case-control study of hospital patients with and without pancreatic cancer published in 198112, which found a strong association between patients’ coffee consumption and their likelihood of having developed pancreatic cancer.\n\n  \n\nObserved Coffee Consumption in Patients With and Without Pancreatic Cancer\n\n\n\nCoffee Consumption\n\n\nPecentage That Drink 0 Cups Per Day\nPecentage That Drink 1 or 2 Cups Per Day\nPecentage That Drink 3+ Cups Per Day\n\n\nDiagnosis\nPancreatic Cancer\n≈ 5% \\(\\left( \\frac{20}{367} \\right)\\)\n≈ 42% \\(\\left( \\frac{153}{367} \\right)\\)\n≈ 53% \\(\\left( \\frac{194}{367} \\right)\\)\n\n\nNot Pacratic Cancer\n≈ 14% \\(\\left( \\frac{88}{643} \\right)\\)\n≈ 42% \\(\\left( \\frac{271}{643} \\right)\\)\n≈ 44% \\(\\left( \\frac{284}{643} \\right)\\)\n\n\n\n\n\nIn the decades since this study was published, several large prospective cohort studies have found no significant relationship between coffee consumption and pancreatic cancer risk13 14 15, suggesting that the original study may have had a flaw in its design. What could it have been?\nIn his seminal epidemiology textbook16, Leon Gordis points out the control group for this study – patients without pancreatic cancer – was chosen in an unusual way. When a patient who did have pancreatic cancer agreed to answer a survey about their dietary habits, control patients were selected by asking the doctor responsible for their care to identify other patients of theirs without the condition. Since these doctors were often gastroenterologists, patients with gastrointestinal disorders were over-represented in the control group. These patients tended to drink less coffee than the average member of the population, either because it exacerbated their symptoms or because they had been advised to cut down their intake by their doctors.\n\nAs a result of the atypical coffee consumption in the control group, the results of this observational study are hard to interpret. This is because even if it was the case that there was no causal relationship between coffee consumption and pancreatic cancer, we would still expect to see an association between them due to the fact that people with GI disorders were more likely to be chosen as controls than other patients. We can show this in a DAG in which ‘Selection into Study’ is a collider."
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#further-reading",
    "href": "posts/observational-studies-and-causal-inference/index.html#further-reading",
    "title": "Observational Studies and Causal Inference",
    "section": "Further Reading",
    "text": "Further Reading\n\nAn introduction to Causal inference – a blog post by Fabian Dablander\nKidney Stones & Simpson’s Paradox – a blog post by Amol Kulkarni"
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#footnotes",
    "href": "posts/observational-studies-and-causal-inference/index.html#footnotes",
    "title": "Observational Studies and Causal Inference",
    "section": "References",
    "text": "References\n\n\nJonathan Wood, “Life of a revolutionary,” OxSciBlog, November 11, 2009.↩︎\n“Study designs,” Centre for Evidence-Based Medicine, October 1, 2020.↩︎\nRobert Matthews, “Storks Deliver Babies (p=0.008),” Teaching Statistics 22, no. 2 (June 2000): 36–38.↩︎\nMartin Bland, An Introduction to Medical Statistics. 4th ed. Oxford: Oxford University Press, 2015.↩︎\nJudea Pearl and Dana Mackenzie, The Book of Why: The New Science of Cause and Effects. London: Penguin Books, 2019.↩︎\nSander Greenland, Judea Pearl, and James Robins, “Causal Diagrams for Epidemiologic Research,” Epidemiology 10, no. 1 (January 1999): 37–48.↩︎\nJudea Pearl, Madelyn Glymour, and Nicholas Jewell, Causal Inference in Statistics: A Primer. New York: Wiley, 2016.↩︎\nJeffrey Aronson, Clare Bankhead, and David Nunan, “Confounding,” Catalogue of Bias, 2018.↩︎\nRicardo Silva, “Causality,” (lecture, Advanced Tutorial Lecture Series on Machine Learning, Cambridge, November 9, 2006).↩︎\nSteven Julious and Mark Mullee, “Confounding and Simpson’s Paradox,” BMJ 309, no. 6968 (December 1994): 1480–1481.↩︎\nClive Charig et al., “Comparison of treatment of renal calculi by open surgery, percutaneous nephrolithotomy, and extracorporeal shockwave lithotripsy,” British Medical Journal (Clinical Research Edition) 292, no. 6524 (March 1986): 879–882.↩︎\nBrian MacMahon et al., “Coffee and cancer of the pancreas,” The New England Journal of Medicine 304, no. 11 (March 1981): 630–633.↩︎\nSimak Bidel et al., “Coffee consumption and risk of gastric and pancreatic cancer – A prospective cohort study,” International Journal of Cancer 312, no.7 (April 2012): 1651–1659.↩︎\nKristin Guertin et al., “A prospective study of coffee intake and pancreatic cancer: results from the NIH-AARP Diet and Health Study,” British Journal of Cancer 113, no. 7 (September 2015): 1081–1085.↩︎\nCharlie Zhou et al., “Coffee and pancreatic cancer risk among never-smokers in the UK prospective Million Women Study,” International Journal of Cancer 145 no. 6 (September 2019); 1484–1492.↩︎\nLeon Gordis, Epidemiology. 4th ed. Philadelphia: Saunders, 2008.↩︎"
  },
  {
    "objectID": "posts/evidence-synthesis-and-forest-plots/index.html",
    "href": "posts/evidence-synthesis-and-forest-plots/index.html",
    "title": "Evidence Synthesis and Forest Plots",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis blog post was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nMeta-analyses and the forest plots they produce are important tools in helping researchers to combine evidence from multiple different studies. In this article, I’ll work through the steps of a fixed-effect meta-analysis, using data from an early and influential systematic review."
  },
  {
    "objectID": "posts/evidence-synthesis-and-forest-plots/index.html#antenatal-steroids",
    "href": "posts/evidence-synthesis-and-forest-plots/index.html#antenatal-steroids",
    "title": "Evidence Synthesis and Forest Plots",
    "section": "Antenatal Steroids",
    "text": "Antenatal Steroids\nIn 1972, an obstetrican and neonatologist working at the National Women’s Hospital in Auckland, New Zealand, published the initial findings from a randomised contolled trial they had begun in 19691. Once all the results were in, this trial showed a significant reduction in the mortality of premature babies whose mothers had been given antenatal steroids, compared to those whose mothers had not received this treatment.\nHowever, one unreplicated trial rarely provides enough evidence for medical practice to change drastically. Over the following decade, another seven, smaller, RCTs of this treatment were carried out, but only two of them showed an effect that was statistically significant at a significance level of 5%. It took a later meta-analysis to show that, when combined, the first eight RCTs of antenatal steroids were in fact sufficient to show a significant effect of antenatal steroids on neonatal mortality in premature babies2.\nThe results of all eight of these studies can be found in a Cochrane review from 19963.\n\n\nInputting the Results of the Eight RCTs into a Data Frame in R\nsteroid_trials &lt;- data.frame(\n  control_group_size = c(538, 61, 59, 75, 58, 71, 63, 372), \n  control_group_deaths = c(60, 5, 7, 7, 12, 10, 11, 34), \n  treatment_group_size = c(532, 69, 67, 71, 64, 56, 81, 371), \n  treatment_group_deaths = c(36, 1, 3, 1, 3, 8, 4, 32))\n\ntrial_names &lt;- c(\"Liggins and Howie (1972)\",\n                 \"Block et al. (1977)\",\n                 \"Morrison et al. (1978)\",\n                 \"Papageorgiou et al. (1979)\",\n                 \"Schutte et al. (1979)\",\n                 \"Taeusch et al. (1979)\",\n                 \"Doran et al. (1980)\",\n                 \"Collaborative Group (1981)\")\n\nrownames(steroid_trials) &lt;- trial_names\n\nnum_trials &lt;- nrow(steroid_trials)\n\ntotals &lt;- rep(NA, times=num_trials)\nfor (i in 1:num_trials) {\n  totals[i] &lt;- steroid_trials$control_group_size[i] +  steroid_trials$treatment_group_size[i]\n}\nsteroid_trials$total_trial_size &lt;- totals\n\n\n\n  \n\nThe Results of the First Eight Randomised Controlled Trials of the Effect of Antenatal Steroids on Neonatal Mortality in Premature Babies\n\n\n\n\n\n\n\n\n\n\n\n\nControl Group\nTreatment Group\nTotal\n\nNumber of\n\nBabies in the Study\n\n\n\nNumber of\n\n\nBabies in the\n\nGroup\n\nNumber of\n\nNeonatal Deaths\n\nNumber of\n\n\nBabies in the\n\nGroup\n\nNumber of\n\nNeonatal Deaths\n\n\nRandomised Control Trial\n\nLiggins and Howie\n\n(1972)\n538\n60\n532\n36\n1070\n\n\n\nBlock et al.\n\n(1977)\n61\n5\n69\n1\n130\n\n\n\nMorrison et al.\n\n(1978)\n59\n7\n67\n3\n126\n\n\n\nPapageorgiou et al.\n\n(1979)\n75\n7\n71\n1\n146\n\n\n\nSchutte et al.\n\n(1979)\n58\n12\n64\n3\n122\n\n\n\nTaeusch et al.\n\n(1979)\n71\n10\n56\n8\n127\n\n\n\nDoran et al.\n\n(1980)\n63\n11\n81\n4\n144\n\n\n\nCollaborative Group\n\n(1981)\n372\n34\n371\n32\n743\n\n\n\n\n\nHere, the years in parentheses after the authors’ names are the years of publication of the first papers they disseminated relating to their trials."
  },
  {
    "objectID": "posts/evidence-synthesis-and-forest-plots/index.html#the-individual-trials-findings",
    "href": "posts/evidence-synthesis-and-forest-plots/index.html#the-individual-trials-findings",
    "title": "Evidence Synthesis and Forest Plots",
    "section": "The Individual Trials’ Findings",
    "text": "The Individual Trials’ Findings\nOnce all the eligible trials have been identified and assessed for bias, the first step of any meta-analysis is to estimate a treatment effect from each of them.\nIn this article, we will use the risk ratio as a measure of the treatment effect. This is a common measure of the association between an exposure and an event, and is equal to the probability of the event in the exposed group divided by the probability of the event in the non exposed group.\nFor example, suppose that a trial produces the following contingency table.\n\n  \n\nExample Contingency Table\n\n\n\nExposed Group\nNon-Exposed Group\n\n\nNumber of Events\n\\(a\\)\n\\(c\\)\n\n\nNumber of Non-Events\n\\(b\\)\n\\(d\\)\n\n\n\n\n\nThen we can find an estimate for the risk ratio using the formula4\n\\[\n\\widehat{\\text{RR}}=\\frac{a}{a+b}\\div\\frac{c}{c+d}.\n\\]\nA risk ratio of \\(1\\) indicates that there is no difference in the probability of the event occurring in the exposed group and the probability of the event occurring in the non-exposed group. Risk ratios are usually plotted on a logarithmic scales in forest plots, so that a change in risk ratio representing a doubling in risk appears equal to a change in risk ratio representing a halving of risk5.\n\n\nCalculating and Plotting the Risk Ratios in R\nrisk_ratios &lt;- rep(NA, times=num_trials)\nfor (i in 1:num_trials) {\n  treatment_incidence &lt;- steroid_trials$treatment_group_deaths[i]/steroid_trials$treatment_group_size[i]\n  control_incidence &lt;- steroid_trials$control_group_deaths[i]/steroid_trials$control_group_size[i]\n  risk_ratios[i] &lt;- treatment_incidence/control_incidence\n}\nsteroid_trials$risk_ratio &lt;- risk_ratios\n\nempty_forest_plot &lt;- function (xax, yax) {\n  plot(1, type=\"n\", log = \"x\", \n     cex.lab=1.5, cex.axis=1.5,\n     xlim=xax, xlab=\"Risk Ratio\", \n     ylim=yax, ylab=\" \", frame.plot = FALSE, yaxt=\"n\")\n  yline &lt;- yax + c(-0.5, 0.5)\n  segments(1, yax[1], 1, yax[2])\n  for (i in 1:num_trials) {\n    y_coord &lt;- num_trials+1-i\n    text(3, y_coord, adj=0, trial_names[i], cex=1.2)\n  }\n}\n\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nempty_forest_plot(xax=c(0.1, 10), yax=c(0.5, 8.5))\npoints(steroid_trials$risk_ratio, 8:1, \n       pch=4, col=\"#006a90\", cex=1.8, lwd=4.2)\n\n\n\n\n\n\n\n\n\nThese point estimates for the risk ratio are not actually very useful on their own, because they do not give information about the different levels of uncertainty surrounding each estimate. For instance, we can tell intuitively that the estimate we found from the study of 1070 babies is likely to be less reliable than the estimate we found from the study of 127 babies.\nTo solve this problem, we can calculate approximate 95% confidence intervals for each study, using the fact that the natural logarithm of our estimator of the risk ratio converges in distribution to a normal distribution with a mean of zero, and a standard deviation that we can approximate using the formula6\n\\[\n\\widehat{\\text{SD}}_{\\text{log}\\widehat{\\text{RR}}}=\\sqrt{n}\\cdot\\text{SE}_{\\text{log}\\widehat{\\text{RR}}}=\\sqrt{n}\\left(\\sqrt{\\frac{1}{a}-\\frac{1}{a+b}+\\frac{1}{c}+\\frac{1}{c+d}}\\right),\n\\] where \\(a\\), \\(b\\), \\(c\\), and \\(d\\) again represent the counts in the contingency table.\n\n\nEstimating and Plotting the Confidence Intervals in R\nconfidence_intervals &lt;- data.frame(\n  lower = rep(NA, times=num_trials),\n  upper = rep(NA, times=num_trials)\n)\nalpha &lt;- 0.05\nz_val &lt;- qnorm(1-alpha/2)\nfor (i in 1:num_trials) {\n  log_rr &lt;- log(steroid_trials$risk_ratio[i])\n  log_rr_SE &lt;- sqrt((1/steroid_trials$control_group_deaths[i]) + (1/steroid_trials$treatment_group_deaths[i])\n                     - (1/steroid_trials$control_group_size[i]) - (1/steroid_trials$treatment_group_size[i]))\n  log_lower &lt;- log_rr - z_val * log_rr_SE\n  log_upper &lt;- log_rr + z_val * log_rr_SE\n  confidence_intervals$lower[i] &lt;- exp(log_lower)\n  confidence_intervals$upper[i] &lt;- exp(log_upper)\n}\n\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\")\nempty_forest_plot(xax=c(0.1, 10), yax=c(0.5, 8.5))\n\nplot_CIs &lt;- function(CIs_df) {\n  num_CIs &lt;- nrow(CIs_df)\n  for (i in 1:num_CIs){\n    y_coord &lt;- num_CIs+1-i\n    lower &lt;- CIs_df$lower[i]\n    upper &lt;- CIs_df$upper[i]\n    if (lower &lt; 0.1) {\n      arrows(x0=0.1, y0=y_coord, \n             x1=upper, y1=y_coord, \n             code=1, length=0.18,\n             lwd=4.2, col=\"#009ed8\")      \n    } else {\n      segments(x0=lower, y0=y_coord, \n               x1=upper, y1=y_coord,\n               lwd=4.2, col=\"#009ed8\")\n    }\n  }\n}\n\nplot_CIs(confidence_intervals)\npoints(steroid_trials$risk_ratio, 8:1, \n       pch=4, col=\"#006a90\", cex=1.8, lwd=4.2)"
  },
  {
    "objectID": "posts/evidence-synthesis-and-forest-plots/index.html#pooling-the-results",
    "href": "posts/evidence-synthesis-and-forest-plots/index.html#pooling-the-results",
    "title": "Evidence Synthesis and Forest Plots",
    "section": "Pooling the Results",
    "text": "Pooling the Results\nNow we want to combine these estimates, giving more weight to the those with lower variance. One way to do this would be to give each estimate a weight proportional to the inverse of its an estimate of its variance, which we could calculate using our previously estimated standard deviation of the log risk ratio. However, this method can be unreliable when the study size is small or the probability of the event we are studying is rare, which is why the Cochrane Collaboration recommends that researchers use the Mantel-Haenszel method for pooling estimates instead7.\nIn this method, each estimate, \\(\\widehat{\\text{RR}}_i\\), is given a weight of8\n\\[\nw_i=\\frac{(a_i+b_i)c_i}{a_i+b_i+c_i+d_i},\n\\]\nwhere for each trial \\(a_i\\) and \\(b_i\\) are the numbers of events and non-events in the exposed group, and \\(c_i\\) and \\(d_i\\) are the numbers of the events and non-events in the non-exposed group. The Mantel-Haenszel estimate is then equal to\n\\[\n\\widehat{\\text{RR}}_{\\text{M-H}}=\\frac{\\sum w_i \\widehat{\\text{RR}}_i}{\\sum w_i}.\n\\]\nWe can find an approximate confidence interval for the Mantel-Haenszel estimator using the asmpymptotic normality of its natural logarithm. A formula for its approximate standard error is given in this guide to standard statistical algorithms used in Cochrane reviews.\n\n\nCalculating a Mantel-Haenszel Pooled Estimate and Confidence Interval for the First Eight Antenatal Steroid RCTs in R\nweights &lt;- rep(NA, num_trials)\nfor (i in 1:num_trials) {\n  treatment_events &lt;- steroid_trials$treatment_group_deaths[i]\n  treatment_non_events &lt;- steroid_trials$treatment_group_size[i] - treatment_events\n  control_events &lt;- steroid_trials$control_group_deaths[i]\n  sample_size &lt;- steroid_trials$total_trial_size[i]\n  weights[i] &lt;- (treatment_events + treatment_non_events) * control_events / sample_size\n}\nsteroid_trials$normalised_weight &lt;- weights / sum(weights)\npooled_rr_estimate &lt;- sum(steroid_trials$risk_ratio * steroid_trials$normalised_weight)\n\n# We find the standard error using the formula from this guide by Jon Deeks and Julian Higgins:\n# https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/meta?action=AttachFile&do=get&target=cochraneor.pdf\nn_1 &lt;- steroid_trials$treatment_group_size\nn_2 &lt;- steroid_trials$control_group_size\na &lt;- steroid_trials$treatment_group_deaths\nc &lt;- steroid_trials$control_group_deaths\nN &lt;- steroid_trials$total_trial_size\nP &lt;- sum((n_1*n_2*(a+c)-a*c*N)/N^2) \nR &lt;- sum(a*n_2/N)\nS &lt;- sum(c*n_1/N)\nlog_SE &lt;- sqrt(P/(R*S))\n\nalpha &lt;- 0.05\nz_val &lt;- qnorm(1-alpha/2)\nlog_CI_lower &lt;- log(pooled_rr_estimate) - z_val * log_SE\nlog_CI_upper &lt;- log(pooled_rr_estimate) + z_val * log_SE\nCI &lt;- exp(c(log_CI_lower, log_CI_upper))\ncat(\"M-H Risk Ratio Estimate: \", pooled_rr_estimate,\n    \",\\n95% Confidence Interval: (\", exp(log_CI_lower), \", \", exp(log_CI_upper), \")\", sep = \"\")\n\n\nM-H Risk Ratio Estimate: 0.6009549,\n95% Confidence Interval: (0.4673707, 0.7727203)\n\n\nOur final pooled estimate for the risk ratio is 0.60 to 2 significant figures. This is equivalent to a 40% reduction in the risk of neonatal death in premature babies whose mothers had been given antenatal steroids.\nTo complete our forest plot, let’s add our Mantel-Haenszel estimate and its confidence interval to our forest plot. By convention, we plot the individual estimates using squares with sizes proportional to their weights, and use a diamond to distinguish our pooled estimate9.\n\n\nCreating the Final Forest Plot in R\nweights &lt;- rep(NA, num_trials)\nfor (i in 1:num_trials) {\n  treatment_events &lt;- steroid_trials$treatment_group_deaths[i]\n  treatment_non_events &lt;- steroid_trials$treatment_group_size[i] - treatment_events\n  control_events &lt;- steroid_trials$control_group_deaths[i]\n  sample_size &lt;- steroid_trials$total_trial_size[i]\n  weights[i] &lt;- (treatment_events + treatment_non_events) * control_events / sample_size\n}\nsteroid_trials$normalised_weight &lt;- weights / sum(weights)\npooled_rr_estimate &lt;- sum(steroid_trials$risk_ratio * steroid_trials$normalised_weight)\n\nn_1 &lt;- steroid_trials$treatment_group_size\nn_2 &lt;- steroid_trials$control_group_size\na &lt;- steroid_trials$treatment_group_deaths\nc &lt;- steroid_trials$control_group_deaths\nN &lt;- steroid_trials$total_trial_size\nP &lt;- sum((n_1*n_2*(a+c)-a*c*N)/N^2) \nR &lt;- sum(a*n_2/N)\nS &lt;- sum(c*n_1/N)\nlog_SE &lt;- sqrt(P/(R*S))\n\nalpha &lt;- 0.05\nz_val &lt;- qnorm(1-alpha/2)\nlog_CI_lower &lt;- log(pooled_rr_estimate) - z_val * log_SE\nlog_CI_upper &lt;- log(pooled_rr_estimate) + z_val * log_SE\nCI &lt;- exp(c(log_CI_lower, log_CI_upper))\n\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\")\nempty_forest_plot(xax=c(0.1, 10), yax=c(-0.5, 8.5))\ntext(3, 0, adj=0, \"Pooled Estimate\", cex=1.2)\n\nplot_CIs(confidence_intervals)\nsegments(x0=CI[1], y0=0, \n         x1=CI[2], y1=0,\n         lwd=4.2, col=\"#009ed8\")\n\nsizes &lt;- steroid_trials$normalised_weight*12\npoints(steroid_trials$risk_ratio, 8:1, \n       pch=15, col=\"#006a90\", cex=sizes, lwd=4.2)\npoints(steroid_trials$risk_ratio, 8:1, \n       pch=\"|\", col=\"#006a90\", cex=2.4)\npoints(pooled_rr_estimate, 0, pch=18, col=\"#006a90\", cex=12)\n\n\n\n\n\n\n\n\n\nA meta-analysis of these eight trials, and several more completed in the intervening years, was eventually published in a systematic review in 198910. This systematic review led directly to the publication of new guidelines by the Royal College of Obstetricians and Gynaecologists in 1992, and antenatal steroids are now routinely given to mothers in premature labour11. Their use is monitored, as a measure of optimal perinatal care, in the National Neonatal Audit Programme run by the Royal College of Paediatrics and Child Health12. The 1989 systematic review also inspired the Cochrane Collaboration’s logo – to learn more, see this page on their website.\nlink"
  },
  {
    "objectID": "posts/evidence-synthesis-and-forest-plots/index.html#further-reading",
    "href": "posts/evidence-synthesis-and-forest-plots/index.html#further-reading",
    "title": "Evidence Synthesis and Forest Plots",
    "section": "Further Reading",
    "text": "Further Reading\n\nA short history of systematic reviews – an article by Cynthia Farquhar and Jane Marjoribanks\nWhat does the Cochrane logo tell us? – a presentation by Steven Woloshin"
  },
  {
    "objectID": "posts/evidence-synthesis-and-forest-plots/index.html#footnotes",
    "href": "posts/evidence-synthesis-and-forest-plots/index.html#footnotes",
    "title": "Evidence Synthesis and Forest Plots",
    "section": "References",
    "text": "References\n\n\nJane Harding and Caroline Crowther, “A history: antenatal corticosteroids,” O&G Magazine 21, no. 1 (March 2019): 13–15.↩︎\n“Prenatal corticosteroids for reducing morbidity and mortality after preterm birth,” in Wellcome Witnesses to Twentieth Century Medicine, volume 25, edited by Tilli Tansey and Lois Reynolds. London: The Wellcome Trust Centre for the History of Medicine at UCL, 2005.↩︎\nPatricia Crowley, “Prophylactic corticosteroids for preterm birth,” Cochrane Database of Systematic Reviews 1996, no. 1.↩︎\nMichael Borenstein et al., Introduction to Meta-Analysis. Chichester, UK: John Wiley & Sons, 2009.↩︎\nJeehyoung Kim, Jay Kaufman, and Heejung Bang, “Graphing Ratio Measures on Forest Plot,” Journal of the America College of Cardiology 71, no. 5 (February 2018): 585–586.↩︎\nAllan Hackshaw, A Concise Guide to Clinical Trials. Chichester, UK: John Wiley & Sons, 2009.↩︎\nJulian Higgins et al., eds., Cochrane Handbook for Systematic Reviews of Interventions, version 6.5. Cochrane, 2024.↩︎\nMathias Harrer et al., Doing Meta-Analysis in R: A Hands-On Guide. Bookdown, 2021.↩︎\nMartin Bland, An Introduction to Medical Statistics. 4th ed. Oxford: Oxford University Press, 2015.↩︎\n“Reducing the play of chance using meta-analysis,” The James Lind Library, April 30, 2024.↩︎\nSense About Science and Academy of Medical Royal Colleges, “Evidence based medicine matters,” Testing Treatments interactive, May 6, 2013.↩︎\nAudit team, “National Neonatal Audit Programme (NNAP) measures,” Royal College of Paediatrics and Child Health, September 2024.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Censored Data and Kaplan-Meier Curves\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic Testing and Bayes’ Theorem\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvidence Synthesis and Forest Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\nKidney Transplants and Graph Theory\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservational Studies and Causal Inference\n\n\n\n\n\n\n\nNo matching items"
  }
]