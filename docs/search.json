[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a maths graduate who’s very interested in the many applications of mathematics and statistics to medicine, and I decided to start this website as a way of challenging myself to keep learning new things (as well as deepening my understanding of areas I already have some knowledge of). It’s a place where I can practise writing expository articles that explore various topics in ‘medical maths’ using diagrams and R code. Thank you for reading – I hope you find my explanations interesting and/or useful!\n\n(If you notice an error or have any other feedback, please feel free to contact me at: medmaths@protonmail.com)\nThis site is built with Quarto and hosted via Github Pages."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis article was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nPatients and medical professionals are often interested in the probability that an event will or will not have occurred after a certain amount of time. For example, what is the probability that a burn will have healed after two weeks? What is the probability that someone’s cancer will not have returned five years after they have gone into remission? To estimate these probabilities, researchers use a branch of statistics known as survival analysis."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#time-to-event-data",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#time-to-event-data",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "Time to Event Data",
    "text": "Time to Event Data\nSurvival analysis is the study of time to event data. These are data that for each subject measure the time that elapses from a defined starting point – for example, the diagnosis of a disease – until a particular event happens – for example, death. Despite the name, survival analysis does not presume that this event is death – indeed, it could be a positive event, such as discharge from hospital. Though commonly used in medical statistics, survival analysis is not exclusive to epidemiology. It is also employed in fields as diverse as engineering and sociology, meaning it is sometimes known by other names, such as ‘reliability analysis’1.\nOne of the central tasks of survival analysis is to estimate the survival function, \\(S(t)\\), for the event of interest. This is a function of time equal to the probability that the event takes longer than the input time, \\(t\\), to occur\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(T\\) denotes a random variable representing the time until the event occurs.\n\\(t\\) denotes a specific time duration.\n\\(S(t)=\\mathbb{P}(T&gt;t)\\) denotes the survival function.\n\n\n\nLet’s consider a very simple scenario in which we might estimate a survival function. Suppose an electrical manufacturing company has a batch of faulty light bulbs, and they decide to pick ten of them at random to leave on and monitor for a week. They observe that:\n\nOn the first day, three light bulbs fail.\nOn the second day, four light bulbs fail.\nOn the third day, no light bulbs fail\nOn the fourth day, two light bulbs fail.\nOn the fifth day, the final light bulb fails.\n\n\nPutting aside the fact that this is a very small sample, it is relatively straightforward to estimate the survival function for these faulty light bulbs from this experiment. For example, to estimate \\(S(1)\\), we just look at the proportion of light bulbs that last longer than one day – seven tenths. By the law of large numbers, as we increase the number of light bulbs in the sample this proportion will eventually converge to the true probability of a faulty light bulb lasting longer than one day. We can do this calculation for each time marker, including the start point of the experiment – we call this Day 0.\n\n  \n\nEstimated Faulty Light Bulb Survival Function\n\n\nDay\nSurvival Function Estimate\n\n\n0\n\\(1\\)\n\n\n1\n\\(\\frac{7}{10}\\)\n\n\n2\n\\(\\frac{3}{10}\\)\n\n\n3\n\\(\\frac{3}{10}\\)\n\n\n4\n\\(\\frac{1}{10}\\)\n\n\n5\n\\(0\\)\n\n\n\n\n\n\n\nPlotting the Estimated Survival Function in R\ndays &lt;- 0:5 # creating a vector of time markers\nsurvival_function &lt;- c(1, 7/10, 3/10, 3/10, 1/10, 0) # creating a vector of estimated probabilities\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nplot(days, survival_function, type=\"s\", ylim=c(0,1), xlab=\"Day\", ylab=\"Estimated Survival Function\", col=\"#006a90\", lwd=4.8, cex.lab=1.8, cex.axis=1.8)\n\n\n\n\n\n\n\n\n\nEasy, right? Unfortunately, the time to event data we find in medical applications are rarely this straightforward to work with."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#censoring",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#censoring",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "Censoring",
    "text": "Censoring\nIn many medical studies producing time to event data, patients are recruited over a period of some time and then followed until a defined end date, at which point the investigators will want to analyse their outcomes. As a result of this, the event of interest may not have happened for all the subjects of the study. In survival analysis, these ‘incomplete’ data points are known as censored data. They do give us some information – we know the event we are studying had not yet happened at the point at which censoring occurred. However, we cannot tell when, or indeed if, it would have gone on to happen.\nCensoring can also come about in other ways, including patients being lost to follow up, or experiencing a ‘competing event’2. For example, in a study of times from catheter placement to complication in acute peritoneal dialysis, observations were considered censored when a catheter was removed before a complication arose3.\nCensored data make estimating a survival function considerably more complicated. Imagine, for example, that three of the light bulbs in our previous thought experiment accidentally got smashed at the end of the first day.\n\nWe could still approximate \\(S(1)\\) to be seven tenths, since \\[\\mathbb{P}(T&gt;1)=1-\\mathbb{P}(T\\leq1),\\] which, since three light bulbs fail on the first day, we can estimate to be \\[1-\\frac{3}{10}=\\frac{7}{10},\\] as in the original case. But what about \\(S(2)\\) onwards? We don’t know on which days the smashed light bulbs would have failed, so we can’t estimate \\(S(t)\\) for these values of \\(t\\) in the same way."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#the-kaplan-meier-method",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#the-kaplan-meier-method",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "The Kaplan-Meier Method",
    "text": "The Kaplan-Meier Method\nIn 1958, Edward Kaplan and Paul Meier proposed an ingenious solution to the censoring problem4. Their paper has gone on to become the most cited publication in the history of statistics5, and is seen as a seminal contribution to the field of survival analysis. Kaplan and Meier’s central insight was that while censoring may stop us from directly estimating \\(\\mathbb{P}(T&gt;t_i)\\) for a particular value of time, \\(t_i\\), it does not stop us from estimating \\[\\mathbb{P}(T&gt;t_i\\,|\\,T&gt;t_{i-1}),\\] This is the conditional probability of an event taking longer than \\(t_i\\) days, months, or years to occur, given that it has not already happened in \\(t_{i-1}\\) days, months, or years.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(t_1,t_2,t_3\\ldots\\), etc. denote the times to the event of interest in ascending order.\n\\(t_{i-1}\\) denotes the next lowest time to the event of interest to \\(t_i\\).\n\\(t_0\\) denotes a time of \\(0\\).\n\n\n\nThese conditional probabilities are quite easy to find, since\n\\[\\begin{split}\n\\mathbb{P}(T&gt;t_i\\,|\\,T&gt;t_{i-1})&=1-\\mathbb{P}(T\\leq t_i\\,|\\,T&gt;t_{i-1})\\\\&=1-\\mathbb{P}(T=t_i\\,|\\,T&gt;t_{i-1}).\n\\end{split}\\]\nwhich we can estimate by calculating the proportion of subjects remaining in the study (those who have not experienced censoring or the event of interest) after \\(t_{i-1}\\) that do not have a time to event of \\(t_i\\).\nIn our smashed light blub thought experiment, for example, we could estimate \\(\\mathbb{P}(T&gt;2\\,|\\,T&gt;1)\\) to be two quarters (or one half). This is because on the first day three light bulbs fail and three light bulbs are censored, so four remain in the experiment on the second day, two of which survive\nKaplan and Meier realised that we can get from these conditional probabilities to the survival function using the chain rule of conditional probabilities. For any \\(t_i\\), we have that\n\\[\n\\begin{split}\n\\mathbb{P}(T&gt;t_i)\n&=\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\mathbb{P}(T&gt;t_{i-1})\\\\\n&=\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\mathbb{P}(T&gt;t_{i-1}\\,|\\,T&gt;t_{i-2})\\mathbb{P}(T&gt;t_{i-2})\\\\\n&=\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\mathbb{P}(T&gt;t_{i-1}\\,|\\,T&gt;t_{i-2})\\mathbb{P}(T&gt;t_{i-2}\\,|\\,T&gt;t_{i-3})\\mathbb{P}(T&gt;t_{i-3})\\cdots\\\\\n&\\cdots\\mathbb{P}(T&gt;t_2\\,|\\,T&gt;t_1)\\mathbb{P}(T&gt;t_2\\,|\\,T&gt;t_1)\\mathbb{P}(T&gt;t_1\\,|\\,T&gt;t_0)\\mathbb{P}(T&gt;t_0).\n\\end{split}\n\\] We know that \\(\\mathbb{P}(T&gt;t_0)\\), the probability of the time to the event of interest being more than zero, is one, since the event has not happened to any of our subjects at the start of the study. So by the expression above, we can recursively estimate the survival function \\(S(t_{i})\\) for each \\(t_1,t_2,t_3,\\ldots\\), using the cumulative product of the conditional probabilities \\(\\mathbb{P}(T&gt; t_i\\,|\\,T&gt;t_{i-1})\\)."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#an-example-with-real-data",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#an-example-with-real-data",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "An Example With Real Data",
    "text": "An Example With Real Data\nTo get an idea of how the Kaplan-Meier method of estimating a survival function works, let’s try it out on some data from a real observational study from the 1980s6. Over two years, a group of obstetricians in London recorded the time it took for a group of 38 women who had been experiencing fertility issues to conceive after a surgical intervention. The times to conception or censoring for each woman can be obtained from a 1998 article on the Kaplan-Meier method in the BMJ7.\n\n\nInputting the Results of the Study into a Data Frame in R\nwomen &lt;- 1:38 # creating identifiers for each woman in the study\nevent_times &lt;- c(1, 1, 11, 7, 3, 4, 9, 4, 24, 6, 9, 2, 6, 16, 9, 2, 2, 3, 1, 8, 8, 4, 4, 1, 7, 9, 2, 9, 24, \n                 1, 1, 9, 13, 10, 3, 2, 3, 2) # inputting the number of months after their surgical intervention\n                                              # at which they conceived or were censored\nconception_or_censoring &lt;- c('Conceived', 'Conceived', 'Censored', 'Censored', 'Conceived', 'Conceived', 'Censored', 'Conceived', 'Censored', 'Conceived', 'Censored',\n                             'Conceived', 'Conceived', 'Conceived', 'Conceived', 'Censored', 'Conceived', 'Conceived', 'Conceived', 'Censored', 'Censored', 'Conceived',\n                             'Censored', 'Conceived', 'Censored', 'Conceived', 'Conceived', 'Conceived', 'Censored', 'Conceived', 'Conceived', 'Censored', 'Conceived',\n                             'Conceived', 'Conceived', 'Conceived', 'Censored', 'Conceived') # inputting which event occurred for each woman\nconception_study &lt;- data.frame(subject=women, months=event_times, event=conception_or_censoring)\n\n\n\n  \n\nTime to Event Data from a Study of Conception after Laparoscopy and Hydrotubation\n\n\n\nStudy Participant\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\nEvent\nConceived\nConceived\nCensored\nCensored\nConceived\nConceived\nCensored\nConceived\nCensored\nConceived\nCensored\nConceived\nConceived\nConceived\nConceived\nCensored\nConceived\nConceived\nConceived\nCensored\nCensored\nConceived\nCensored\nConceived\nCensored\nConceived\nConceived\nConceived\nCensored\nConceived\nConceived\nCensored\nConceived\nConceived\nConceived\nConceived\nCensored\nConceived\n\n\nMonths Until Event\n1\n1\n11\n7\n3\n4\n9\n4\n24\n6\n9\n2\n6\n16\n9\n2\n2\n3\n1\n8\n8\n4\n4\n1\n7\n9\n2\n9\n24\n1\n1\n9\n13\n10\n3\n2\n3\n2\n\n\n\n\n\nTo make it easier to do our survival analysis, let’s summarise this data so that we can easily see the number of women who conceived or were censored at each distinct time step.\n\n\nRestructuring the Data in R\nmonths_from_intervention &lt;- sort(unique(conception_study$months)) # getting the different numbers of months after surgical intervention \n                                                                  # it took for conception or censoring to occur\nmonths_from_intervention &lt;- c(0, months_from_intervention) # adding month zero for the start of the study\nnum_times &lt;- length(months_from_intervention) # finding the number of unique event times \ncensored &lt;- rep(NA, times=num_times) \nconceived &lt;- rep(NA, times=num_times)\nfor (i in 1:num_times) { # counting the number of conceptions and censorings at each event time\n  month_i &lt;- months_from_intervention[i]\n  censored[i] &lt;- nrow(conception_study[conception_study$months == month_i & conception_study$event == 'Censored', ])\n  conceived[i] &lt;- nrow(conception_study[conception_study$months == month_i & conception_study$event == 'Conceived', ])\n}\nconception_data_summary &lt;- data.frame(months=months_from_intervention, censored=censored, conceived=conceived)\n\n\n\n  \n\nSummarised Conception Study Data\n\n\nMonths from Intervention\nNumber of Censorings\nNumber of Conceptions\n\n\n0\n0\n0\n\n\n1\n0\n6\n\n\n2\n1\n5\n\n\n3\n1\n3\n\n\n4\n1\n3\n\n\n6\n0\n2\n\n\n7\n2\n0\n\n\n8\n2\n0\n\n\n9\n3\n3\n\n\n10\n0\n1\n\n\n11\n1\n0\n\n\n13\n0\n1\n\n\n16\n0\n1\n\n\n24\n2\n0\n\n\n\n\n\nTo calculate the conditional probability \\(\\mathbb{P}(T&gt;t_i|T&gt;t_{i-1})\\) for each observed number of months from conception, \\(t_i\\), we first need to find out how many women were still being followed up at each stage.\n\n\nFinding the Number of Remaining Study Members at Each Time Stage in R\nnum_remaining &lt;- rep(NA, times=num_times) # initialising a vector to store the number of remaining study members at each event time\nnum_remaining[1] &lt;- 38 # at month zero, all women are still being followed up \nfor (i in 2:num_times) { # finding the number of women still being followed up at each event time from the previous number, \n                         # and the number who conceived or were censored at the previous event time\n  num_remaining[i] &lt;- num_remaining[i-1] - censored[i-1] - conceived[i-1]\n}\nconception_data_summary$remaining &lt;- num_remaining # adding a column to the data frame\n\n\n\n  \n\nSummarised Conception Study Data (Including Numbers of Participants Remaining in Follow Up)\n\n\nMonths from Intervention\nNumber of Censorings\nNumber of Conceptions\nNumber Still in Study\n\n\n0\n0\n0\n38\n\n\n1\n0\n6\n38 − 0 − 0 = 38\n\n\n2\n1\n5\n38 − 0 − 6 = 32\n\n\n3\n1\n3\n32 − 1 − 5 = 26\n\n\n4\n1\n3\n26 − 1 − 3 = 22\n\n\n6\n0\n2\n22 − 1 − 3 = 18\n\n\n7\n2\n0\n18 − 0 − 2 = 16\n\n\n8\n2\n0\n16 − 2 − 0 = 14\n\n\n9\n3\n3\n14 − 2 − 0 = 12\n\n\n10\n0\n1\n12 − 3 − 3 = 6\n\n\n11\n1\n0\n6 − 0 − 1 = 5\n\n\n13\n0\n1\n5 − 1 − 0 = 4\n\n\n16\n0\n1\n4 − 0 − 1 = 3\n\n\n24\n2\n0\n3 − 0 − 1 = 2\n\n\n\n\n\nNow we can start our survival analysis properly by estimating \\(\\mathbb{P}(T&gt;t_i\\,|\\,T&gt;t_{i-1})\\) for each \\(t_i\\). We have gleaned all the information we need from the number of censorings at each time stage, so this column can be removed.\n\n\nEstimating the Conditional Probabilities for Each Time Stage in R\nconception_survival_analysis &lt;- subset(conception_data_summary, select=-censored) # creating a new data frame without the censoring column\nconditional_probabilities &lt;- rep(NA, times=num_times) # initialising a vector to store the estimated conditional probabilities\nfor (i in 1:num_times) { # finding an estimate for the conditional probability at each event time\n  conditional_probabilities[i] &lt;- (num_remaining[i]-conceived[i])/num_remaining[i]\n}\nconception_survival_analysis$conditionals &lt;- conditional_probabilities\n\n\n\n  \n\nConception Study Data With Estimated Conditional Probabilities\n\n\n\\(i\\)\n\\(t_{i}\\) (Months)\n\\(\\#(T = t_{i})\\) (Conceptions)\nNumber Still in Study\nEstimated \\(\\mathbb{P}(T &gt; t_{i}\\,|\\, T &gt; t_{i - 1})\\)\n\n\n0\n0\n0\n38\n\\(\\text{N/A}\\,(\\mathbb{P}(T &gt; 0) = 1)\\)\n\n\n1\n1\n6\n38\n\\(1 - \\frac{6}{38} = \\frac{32}{38} \\approx 0.84\\)\n\n\n2\n2\n5\n32\n\\(1 - \\frac{5}{32} = \\frac{27}{32} \\approx 0.84\\)\n\n\n3\n3\n3\n26\n\\(1 - \\frac{3}{26} = \\frac{23}{26} \\approx 0.88\\)\n\n\n4\n4\n3\n22\n\\(1 - \\frac{3}{22} = \\frac{19}{22} \\approx 0.86\\)\n\n\n5\n6\n2\n18\n\\(1 - \\frac{2}{18} = \\frac{16}{18} \\approx 0.89\\)\n\n\n6\n7\n0\n16\n\\(1 - \\frac{0}{16} = 1\\)\n\n\n7\n8\n0\n14\n\\(1 - \\frac{0}{14} = 1\\)\n\n\n8\n9\n3\n12\n\\(1 - \\frac{3}{12} = \\frac{9}{12} = 0.75\\)\n\n\n9\n10\n1\n6\n\\(1 - \\frac{1}{6} = \\frac{5}{6} \\approx 0.83\\)\n\n\n10\n11\n0\n5\n\\(1 - \\frac{0}{5} = 1\\)\n\n\n11\n13\n1\n4\n\\(1 - \\frac{1}{4} = \\frac{3}{4} = 0.75\\)\n\n\n12\n16\n1\n3\n\\(1 - \\frac{1}{3} = \\frac{2}{3} \\approx 0.67\\)\n\n\n13\n24\n0\n2\n\\(1 - \\frac{0}{2} = 1\\)\n\n\n\n\n\nFinally, we can use these conditional probabilities to estimate \\(S(t)\\).\n\n\nEstimating the Survival Function in R\nsurvival_function &lt;- cumprod(conditional_probabilities) # finding the cumulative product of the estimated conditional probabilities\nconception_survival_analysis$survival &lt;- survival_function\n\n\n\n  \n\nSurvival Analysis of Time to Event Data from a Study of Conception after Laparoscopy and Hydrotubation\n\n\n\\(i\\)\n\\(t_{i}\\) (Months)\n\\(\\#(T = t_{i})\\) (Conceptions)\nNumber Still in Study\nEstimated \\(\\mathbb{P}(T &gt; t_{i}\\,|\\, T &gt; t_{i - 1})\\)\nEstimated \\(\\mathbb{P}(T &gt; t_{i})\\)\n\n\n0\n0\n0\n38\n\\(\\text{N/A}\\,(\\mathbb{P}(T &gt; 0) = 1)\\)\n\\(1\\)\n\n\n1\n1\n6\n38\n\\(\\frac{32}{38}\\)\n\\(\\frac{32}{38} \\times 1 = \\frac{32}{38} \\approx 0.84\\)\n\n\n2\n2\n5\n32\n\\(\\frac{27}{32}\\)\n\\(\\frac{27}{32} \\times \\frac{32}{38} = \\frac{27}{38} \\approx 0.71\\)\n\n\n3\n3\n3\n26\n\\(\\frac{23}{26}\\)\n\\(\\frac{23}{26} \\times \\frac{27}{38} = \\frac{621}{988} \\approx 0.63\\)\n\n\n4\n4\n3\n22\n\\(\\frac{19}{22}\\)\n\\(\\frac{19}{22} \\times \\frac{621}{988} = \\frac{621}{1144} \\approx 0.54\\)\n\n\n5\n6\n2\n18\n\\(\\frac{16}{18}\\)\n\\(\\frac{16}{18} \\times \\frac{621}{1144} = \\frac{69}{143} \\approx 0.48\\)\n\n\n6\n7\n0\n16\n\\(1\\)\n\\(1 \\times \\frac{69}{143} = \\frac{69}{143} \\approx 0.48\\)\n\n\n7\n8\n0\n14\n\\(1\\)\n\\(1 \\times \\frac{69}{143} = \\frac{69}{143} \\approx 0.48\\)\n\n\n8\n9\n3\n12\n\\(\\frac{9}{12}\\)\n\\(\\frac{9}{12} \\times \\frac{69}{143} = \\frac{207}{572} \\approx 0.36\\)\n\n\n9\n10\n1\n6\n\\(\\frac{5}{6}\\)\n\\(\\frac{5}{6} \\times \\frac{207}{572} = \\frac{345}{1144} \\approx 0.30\\)\n\n\n10\n11\n0\n5\n\\(1\\)\n\\(1 \\times \\frac{345}{1144} = \\frac{345}{1144} \\approx 0.30\\)\n\n\n11\n13\n1\n4\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4} \\times \\frac{345}{1144} = \\frac{1035}{4576} \\approx 0.23\\)\n\n\n12\n16\n1\n3\n\\(\\frac{2}{3}\\)\n\\(\\frac{2}{3} \\times \\frac{1035}{4576} = \\frac{345}{2288} \\approx 0.15\\)\n\n\n13\n24\n0\n2\n\\(1\\)\n\\(1 \\times \\frac{345}{2288} = \\frac{345}{2288} \\approx 0.15\\)\n\n\n\n\n\nWe can now plot our estimate for \\(\\mathbb{P}(T&gt;t)\\) as a stepwise function of \\(t\\). As is conventional when plotting Kaplan-Meier curves we also mark the months in which censoring occurs.\n\n\nPlotting the Estimated Survival Function in R\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nplot(conception_survival_analysis$months, conception_survival_analysis$survival, type=\"s\", ylim=c(0,1), \n     xlab=\"Months\", ylab=\"Estimated Survival Function\", col=\"#009ed8\", lwd=4.8, cex.lab=1.8, cex.axis=1.8)\ncensoring_months &lt;- sort(unique(conception_study[which((conception_study$event == 'Censored')),]$months))  # finding the months in which censoring occurred\nsurv_fuc_eval_at_censoring &lt;- conception_survival_analysis[conception_survival_analysis$months %in% censoring_months,]$survival # evaluating the estimated survival function \n                                                                                                                                # at those points\npoints(censoring_months, surv_fuc_eval_at_censoring, pch=4, cex=1.8, col=\"#006a90\", lwd=4.8)\n\n\n\n\n\n\n\n\n\nOur estimated survival function allows us to work out a principled estimate for the median time to conception after laparoscopy and hydrotubation. Since the median time it takes to conceive, \\(t_m\\), is equal to the point where \\(\\mathbb{P}(T&gt;t_m)=0.5\\), we can just draw a line across from 0.5 on the y-axis to our Kaplan-Meier curve.\n\n\nFinding the Median Time to Conception in R\nmedian_month &lt;- conception_survival_analysis$months[which(conception_survival_analysis$survival &lt;= 0.5)[1]] # finding the first month at which the estimated survival function drops to equal or below 0.5\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nplot(conception_survival_analysis$months, conception_survival_analysis$survival, type=\"s\", ylim=c(0,1), xlab=\"Months\", ylab=\"Estimated Survival Function\", col=\"#009ed8\", lwd=4.8, cex.lab=1.8, cex.axis=1.8)\npoints(censoring_months, surv_fuc_eval_at_censoring, pch=4, cex=1.8, col=\"#006a90\", lwd=4.8)\nsegments(-1, 0.5, median_month, 0.5, col=\"#ffc000\", lwd=4.8, lty=2) # plotting the line across from the y-axis to the survival curve\naxis(1, at=median_month, cex.lab=1.8, cex.axis=1.8) # marking 0.5 on the y-axis\nsegments(median_month, -1, median_month, 0.5, col=\"#ffc000\", lwd=4.8, lty=2) # plotting the line down from the survival curve to the x-axis\naxis(2, at=0.5, cex.lab=1.8, cex.axis=1.8) # marking the median on the x-axis\n\n\n\n\n\n\n\n\n\nWe find that the median number of months that passed before women conceived after laparoscopy and hydrotubation was six. So, based on the results of this study, we would expect a woman to have a 50% chance of conceiving within the first six months following laparoscopy and hydrotubation."
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#further-reading",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#further-reading",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "Further Reading",
    "text": "Further Reading\n\nWhat is the Kaplan Meier curve? – a blog post by Steve Simon\nWhat is survival analysis? – a blog post by Antoine Soetewey"
  },
  {
    "objectID": "posts/censored-data-and-kaplan-meier-curves/index.html#footnotes",
    "href": "posts/censored-data-and-kaplan-meier-curves/index.html#footnotes",
    "title": "Censored Data and Kaplan-Meier Curves",
    "section": "References",
    "text": "References\n\n\nEmily Zabor, “Survival Analysis in R,” June 21, 2023.↩︎\nDouglas Altman and Martin Bland, “Time to event (survival) data,” BMJ 317, no. 7156 (August 1998): 468–469.↩︎\nVimal Chadha et al., “Tenckhoff catheters prove superior to cook catheters in pediatric acute peritoneal dialysis,” American Journal of Kidney Diseases 35, no. 6 (June 2000): 111–1116.↩︎\nEdward Kaplan and Paul Meier, “Nonparametric Estimation From Incomplete Observations,” Journal of the American Statistical Association 53, no. 282 (June 1958): 45–481.↩︎\nRichard Van Noorden, Brendan Maher, and Regina Nuzzo, “The Top 100 Papers,” Nature 514, no. 7524 (October 2014): 55–553.↩︎\nParamjit Luthra, Martin Bland, and Stuart Stanton, “Incidence of Pregnancy After Laparoscopy and Hydrotubation,” British Medical Journal (Clinical Research Edition) 284, no. 6321 (April, 1982): 101.↩︎\nMartin Bland and Douglas Altman, “Survival probabilities (the Kaplan-Meier method),” BMJ 317, no. 7172 (December 1998): 1572–1580.↩︎"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis article was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nIf I am screened for a disease and receive a positive test result, how worried should I be? What are the chances it was a false positive? In this article I’ll discuss why medical test results aren’t always as straightforward as they might seem, and how an equation first discovered over 200 years ago can help us to understand them better."
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#sensitivity-and-specificity",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#sensitivity-and-specificity",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\nWhen interpreting a diagnostic test result, two of most important pieces of information we need to know are the sensitivity and specificity of the test. Coined in 1947 by Jacob Yerushalmy1, these terms refer respectively to the probability that a diagnostic test will correctly identify someone with a disease, and the probability that it will accurately distinguish those without the condition.\nSteven McGee gives a helpful illustration of these concepts in his book Evidence-Based Physical Diagnosis2, using a hypothetical experiment. He asks us to imagine that a specific physical examination finding – a holosystolic heart murmur – is assessed in a group of 42 people with a particular type of valvular heart disease and a group of 58 people without the condition, producing the following results.\n\n  \n\nPhysical Examination Findings in Patients With and Without Tricuspid Regurgitation\n\n\n\nValvular Heart Disease Status\n\n\nPatients With Tricuspid Regurgitation\nPatients Without Tricuspid Regurgitation\n\n\nPhysical Examination Finding\nHolosystolic Murmur Present\n22\n3\n\n\nHolosystolic Murmur Absent\n20\n55\n\n\n\n\n\nThe sensitivity of a test for a disease is the proportion of patients who have the condition and test positive. To calculate it we can divide the number of true positives by the sum of the number of true positives and the number of false negatives.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(\\text{FP}\\) denotes the number of false positives.\n\\(\\text{FN}\\) denotes the number of false negatives.\n\\(\\text{TP}\\) denotes the number of true positives.\n\\(\\text{TN}\\) denotes the number of true negatives.\n\n\n\nFor the example above this gives us a specificity of\n\\[\n\\text{Sensitivity}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}=\\frac{22}{22+20}=\\frac{22}{42}\\approx0.52,\n\\] equivalent to around 52%. We can find the specificity – the proportion of patients without the disease who correctly test negative – similarly;\n\\[\n\\text{Specificity}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}=\\frac{22}{55+3}=\\frac{55}{58}\\approx0.95.\n\\]"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#the-base-rate-fallacy",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#the-base-rate-fallacy",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "The Base Rate Fallacy",
    "text": "The Base Rate Fallacy\nNow we’re clear on the definitions of sensitivity and specificity, let’s look at a common way in which these values can be misinterpreted. Imagine there is a medical condition, let’s call it Bayesitis, with a prevalence of 5% – meaning it affects one in 20 people. Now suppose that a blood test for Bayesitis is found to have a sensitivity of 99% and a specificity of 85%. If a random member of the population has their blood tested and receives a positive result, what is the probability that it is a false positive and they don’t actually have the disease?\nWe might intuitively assume that the answer is 15% since, according to the definition of specificity, this is the proportion of patients without Bayesitis who will test positive incorrectly. Unfortunately, by making this assumption, we are falling into the trap of the base rate fallacy.\nLet’s see if we can get a ballpark figure for the true probability by thinking through what would happen if we were to screen a representative sample of 20 people for Bayesitis. We’ll assume that one person in our sample – 5% – has the disease. Since the sensitivity of our blood test is 99% this person is very likely to test positive, giving us one true positive. Now, what about the 19 people in our sample who do not have the disease? Referring to the specificity of our blood test, we can expect it to correctly identify 85% percent of them, giving us 16.15 true negatives. Since we can’t have 0.15 of a person, let’s round this to 16, which leaves us with three false positives.\n\nSo three out of four positive test results from our representative sample are false, meaning that if someone from our sample receives a positive test result, the probability that they don’t actually have Bayesitis is 75%. This is far enough from our initial guess of 15% for us to know that we must have made an error in our reasoning.\nOur problem was that we failed to take into account the population prevalence, or base rate, of Bayesitis. To see the importance of the base rate, consider a group of people in which 0% have the condition – in a population like this, there is a 100% probability that any positive test results are false.\nClearly, the sensitivity and specificity of a test aren’t on their own sufficient for us to calculate the probability that any particular positive test result is false. How, then, do we find the exact value of this probability?"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#bayes-to-the-rescue",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#bayes-to-the-rescue",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Bayes to the Rescue",
    "text": "Bayes to the Rescue\nThe answer, of course, is Bayes’ Theorem. Discovered by Thomas Bayes in the 1740s and given its modern form by Pierre Simon Laplace in the 1810s3, this famous theorem states that for any two events \\(A\\) and \\(B\\), we can get the conditional probability of \\(A\\) given \\(B\\) from the conditional probability of \\(B\\) given \\(A\\) using the formula\n\\[\n\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)},\n\\] where \\(\\mathbb{P}(A)\\) and \\(\\mathbb{P}(B)\\) are the marginal (unconditional) probabilities of \\(A\\) and \\(B\\) respectively.\nIn our example, \\(A\\) corresponds to the event that the person who had their blood tested actually has Bayesitis, and \\(B\\) corresponds to the event that they test positive for the disease. When we committed the base rate fallacy, we made the mistake of assuming that \\(\\mathbb{P}(A|B)\\) was equivalent to \\(\\mathbb{P}(B|A)\\), and we failed to consider the marginal probabilities \\(\\mathbb{P}(A)\\) and \\(\\mathbb{P}(B)\\). Let’s try to work out the probability that their test result was a false positive again, this time using Bayes’ theorem.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(D^{-}\\) denotes not having the disease.\n\\(D^{+}\\) denotes having the disease.\n\\(T^{-}\\) denotes testing negative.\n\\(T^{+}\\) denotes testing positive\n\n\n\nWe want to find\n\\[\n\\mathbb{P}(D^{-}|T^{+})=\\frac{\\mathbb{P}(T^{+}|D^{-})\\mathbb{P}(D^{-})}{\\mathbb{P}(T^{+})},\n\\]\nWe know that \\(\\mathbb{P}(D^{-})=0.95\\), equivalent to 95%, since the prevalence of Bayesitis in the population is 95%. We also know \\(\\mathbb{P}(T^{+}|D^{-})=0.15\\), as we defined our imaginary test to be 85% accurate. So to find the probability of a positive test result being false we just need to find the denominator of the fraction above; \\(\\mathbb{P}(T^{+})\\), the overall probability of testing positive.\nWe can write\n\\[\n\\begin{split}\n\\mathbb{P}(T^{+})&=\\mathbb{P}(T^{+}\\cap(D^{+}\\cup D^{-}))\n\\\\&=\\mathbb{P}(T^{+}\\cap D^{+})+\\mathbb{P}(T^{+}\\cap D^{-})\n\\\\&=\\mathbb{P}(T^{+}|D^{+})\\mathbb{P}(D^{+})+\\mathbb{P}(T^{+}|D^{-})\\mathbb{P}(D^{-}),\n\\end{split}\n\\] using the fact that \\(D^{+}\\) and \\(D^{-}\\) are collectively exhaustive and mutually exclusive events, as well as the definition of conditional probability, which we can rearrange to get that \\(\\mathbb{P}(A\\cap B)=\\mathbb{P}(A|B)\\mathbb{P}(B)\\).\nNow, to work out \\(\\mathbb{P}(T^{+})\\), we can use our knowledge of the sensitivity and specificity of our blood test and the prevalence of Bayesitis to subsitute in the values \\(\\mathbb{P}(T^{+}|D^{+})=0.99\\), \\(\\mathbb{P}(D)=0.05\\), \\(\\mathbb{P}(T^{+}|D^{-})=0.15\\), and \\(\\mathbb{P}(D^{-})=0.95\\), finding that\n\\[\n\\mathbb{P}(T^{+})=0.99\\times 0.05 + 0.15\\times 0.95 = 0.192.\n\\] Finally, we can calculate that\n\\[\n\\mathbb{P}(D^{-}|T^{+})=\\frac{0.15\\times0.95}{0.192}=0.7421875,\n\\] which is pretty close to the estimate we got from our thought experiment involving 20 people!\nTo check our answer, we can use R to simulate repeatedly sampling 20 people from a population of 1 million people with a 5% prevalence of Bayesitis.\n\n\nSimulating Bayesitis Screening in R\n# Because we will be using randomly generated numbers, we first set a seed to ensure our results are reproducible:\nset.seed(31415926) \n# We now set up some variables:\npop_size &lt;- 1000000\nbayesitis_prevalence &lt;- 0.05\nbayesitis_pop_size &lt;- pop_size * bayesitis_prevalence # we calculate 5% of the population to find the number of people that have Bayesitis\n# Now imagine that each person in the population is assigned a number randomly pick 5% of these numbers to designate the people that have Bayesitis\ntotal_pop &lt;- 1:pop_size # we create a vector made up of the numbers from 1 to 1,000,000\nbayesitis_pop &lt;- sample(total_pop, size=bayesitis_pop_size, replace=FALSE) # we randomly samples 5% of the entries from our total_pop vector\n# Let's create a logical vector (one that only contains TRUE and FALSE values) to represent the Bayesitis status of every member of the population:\nbayesitis_statuses &lt;- rep(FALSE, times=pop_size) # we initialise a vector the length of the population where every entry is FALSE\nbayesitis_statuses[bayesitis_pop] &lt;- TRUE # we change the entries corresponding to the people with Byesitis to TRUE\n# We can now set up our simulations: \nnum_sims &lt;- 300\nfalse_pos_props &lt;- rep(NA, times=num_sims) # we initialise a vector to store the proportion of false positives we get from each simulation \n# Now let's repeatedly pick 20 random people from the population and simulate screening them for Bayesitis:\nsamp_size &lt;- 20\nfor (n in 1:num_sims) {\n  rand_samp &lt;- sample(total_pop, size=samp_size, replace=FALSE)\n  # Let's set up variables to store the number of true and false positive test results:\n  num_true_pos &lt;- 0\n  num_false_pos &lt;- 0\n  rand_nums &lt;- runif(samp_size, min=0, max=1) # we generate 20 random numbers between 0 and 1, so that everyone in the sample gets a correct/incorrect\n                                              # test result with a probability according to the sensitivity and specificty of our test\n  for (i in 1:samp_size) {\n    true_status &lt;- bayesitis_statuses[rand_samp[i]]\n    if (rand_nums[i] &lt;= 0.99 & true_status == TRUE) { # if the person does have Bayesitis they get an accurate positive result with a probability of 99%\n      num_true_pos &lt;- num_true_pos + 1\n    } else if (rand_nums[i] &gt; 0.85 & true_status == FALSE) { # if they don't they get a false positive result with a probability of 15%\n      num_false_pos &lt;- num_false_pos + 1\n    }\n  }\n  total_pos &lt;- num_true_pos + num_false_pos\n  if (total_pos != 0) { # if there are no positive test results, true or false, the proportion of positive test results that are false is undefined\n    false_pos_props[n] &lt;- num_false_pos / total_pos\n  }  \n}\n# Now let's see how the average proportion of positive test results that are false changes as the number of simulations we've done increases:\nprops_for_avg &lt;- false_pos_props[!is.na(false_pos_props)] # we remove the undefined values from our vector of empirical false positive probabilities\ncum_avg &lt;- cumsum(props_for_avg) / seq_along(props_for_avg) # we calculate a cumulative average as the number of simulations increases\n# We plot the cumulative average against the number of simulations:\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\")\nplot(cum_avg, ylim=c(0.5,1), xlab=\"Number of Simulations\", ylab=\"Average False Positive Proportion\", type=\"l\", col=\"#808080\", lwd=4.2, cex.lab=1.5, cex.axis=1.5)\nabline(h=0.7421875, col=\"#009ed8\", lty=1, lwd=4.2)\nlegend(\"topright\", legend=\"Calculated Probability\", col=\"#009ed8\", lty=1, lwd=4.2, cex=1.2)\n\n\n\n\n\n\n\n\n\nReassuringly, as we do more simulations, the average proportion of positive test results that are false appears to converge to our calculated value of \\(\\mathbb{P}(D^{-}|T^{+})\\)."
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#further-reading",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#further-reading",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Further Reading",
    "text": "Further Reading\n\nBayes theorem and likelihood ratios for diagnostic tests – a blog post by Stathis Kamperis about likelihood ratios\nAn Interactive Fagan Nomogram – a blog post by Carlos Scheidegger about the Fagan nomogram"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#footnotes",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#footnotes",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "References",
    "text": "References\n\n\nJacob Yerushalmy, “Statistical problems in assessing methods of medical diagnosis with special reference to x-ray techniques,” Public Health Reports 62, no. 2 (October 1947): 1432–1439.↩︎\nSteven McGee, Evidence-Based Physical Diagnosis. 3rd ed. Philadelphia: Saunders, 2012.↩︎\nSharon Bertsch McGrayne, The Theory That Would Not Die. New Haven: Yale University Press, 2011.↩︎"
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html",
    "href": "posts/observational-studies-and-causal-inference/index.html",
    "title": "Observational Studies and Causal Inference",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis blog post was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nRandomised Controlled Trials (RCTs) are widely recognised as the gold standard of study designs, but they are not always possible to perform. For example, it would have been neither ethical nor feasible for Richard Doll and Austin Bradford Hill to randomly assign a group of volunteers to a smoking and non-smoking group in order to show that cigarettes cause lung cancer. However, they were still able to establish a link between smoking and the disease, using first a case-control study and then a prospective cohort study1.\nThese are both types of observational study2, an alternative to RCTs that can be very useful – as in the case of smoking and lung cancer. Unfortunately, the fact the participants are not randomised to different groups in observational studies can make their results particularly difficult to interpret. In this article, I will discuss how diagrams representing causal relationships can help us to spot potential sources of bias in these kinds of studies."
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#causal-inference",
    "href": "posts/observational-studies-and-causal-inference/index.html#causal-inference",
    "title": "Observational Studies and Causal Inference",
    "section": "Causal Inference",
    "text": "Causal Inference\nCausal relationships are notoriously difficult to distinguish from mere statistical associations, as summed up in the famous saying “correlation does not imply causation”. A humorous example of this can be found in an educational statistics paper in which the author investigates the fairy tale that storks deliver babies by analysing 1980s estimates of stork populations and birth rates in seventeen different European countries3.\n\n  \n\nEstimates of European Stork Populations and Birth Rates in the 1980s\n\n\n\nCountry\n\n\nAlbania\nAustria\nBelgium\nBulgaria\nDenmark\nFrance\nGermany\nGreece\nHolland\nHungary\nItaly\nPoland\nPortugal\nRomania\nSpain\nSwitzerland\nTurkey\n\n\nStork Pairs\n100\n300\n1\n5000\n9\n140\n3300\n2500\n4\n5000\n5\n30,000\n1500\n5000\n8000\n150\n25,000\n\n\nYearly Births\n83,000\n87,000\n118,000\n117,000\n59,000\n774,000\n901,000\n106,000\n188,000\n124,000\n551,000\n610,000\n120,000\n367,000\n439,000\n82,000\n1,576,000\n\n\n\n\n\nWe can plot these data and observe a linear relationship between stork populations and birth rates, with a correlation coefficient of 0.62 to two significant figures.\n\n\nPlotting the Data and Calculating the Correlation Coefficient in R\nstork_pairs &lt;- c(100, 300, 1, 5000, 9, 140, 3300, 2500, 4, 5000, 5, 30000, 1500, 5000, 8000, 150, 25000)\nyearly_births &lt;- c(83000, 87000, 118000, 117000, 59000, 774000, 901000, 106000, 188000, 124000, 551000, 610000, 120000, 367000, 439000, 82000, 1576000)\ncorrelation &lt;- cor(stork_pairs, yearly_births) # calculating the pearson correlation coefficient\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") # setting the plot options\nplot(stork_pairs, yearly_births, # plotting the data points\n     xlab=\"Stork Pairs\", ylab=\"Yearly Births\", \n     pch=4, cex=1.8, col=\"#009ed8\", lwd=4.8, cex.lab=1.5, cex.axis=1.5)\nabline(lm(yearly_births ~ stork_pairs), # adding the line of best fit\n       col = \"#ffc000\", lty=2, lwd = 4.2)\nlegend(\"topleft\", legend=paste0(\"Line of Best Fit  (r=\", round(correlation, 2), \")\"), \n       col=\"#ffc000\", lty=2, lwd=4.2, cex=1.5)\n\n\n\n\n\n\n\n\n\nMoreover, performing a significance test for correlation with the null hypothesis that there is no relationship between stork populations and birth rates gives us a p-value of less than 0.05, a common threshold for a result to be considered statistically significant4.\n\n\nTesting the Null Hypothesis in R\nsignificance_test &lt;- cor.test(stork_pairs, yearly_births)\np_val &lt;- significance_test$p.value\nprint(paste(\"p-value:\", p_val), quote=FALSE)\n\n\n[1] p-value: 0.00789786078801809\n\n\nClearly, more storks do not cause more babies to be born. But the non-causality of statistical associations might not always be so obvious. To deal with the challenges of understanding causes and effects, a new branch of statistics known as ‘causal inference’ has emerged over the past few decades5.\nThis type of statistics often involves the use of directed acyclic graphs – ‘DAGs’ for short – to model the relationships between variables. Let’s explore the essential features of these diagrams.\n\nDirected Acyclic Graphs\nIn discrete mathematics, a graph is a structure made up of vertices linked by edges. A path between two vertices is a sequence of edges linking them together6.\n\nFor example, in the above graph, there is a path between A and C along the the sequence of edges between vertices A, B, D, E, and C.\nDirected graphs are a particular type of graph in which all the edges are arrows. In these types of graph a vertex can be an descendant or ancestor of another vertex if there is a path of arrows all pointing in the same direction between them; the descendant lies at the arrowhead and ancestors lies at the tail. If this path is only one edge long, the ancestor can be called the descendant’s parent and the descendant can be called the parent’s child7.\n\nHere, B is a parent (and thus also an ancestor) of C and D, a child (and descendant) of A, and and ancestor of E.\nDirected graphs are ideal for causal inference, because we can use vertices to represent the variables and edges to represent the causal relationships between them, with parents having a direct effect on their children. For example, a simplified DAG showing the effects of different factors on a person’s chance of developing lung cancer might show the variables ‘Genetics’, ‘Environmental Factors’, and ‘Smoking’ all to be parents of the variable ‘Lung Cancer Risk’.\n\nAn acyclic graph is simply one that contains no cycles (a cycle is a directed path leading from a variable back to itself), so when we use a DAG to model causal relationships a variable cannot cause itself.\nAn example of a DAG which might explain the correlation between the stork populations and birth rates of European countries could introduce extra variables, such as land area and human population size.\n\nThis DAG could account for the association between the ‘Stork Population’ and ‘Birth Rate’ variables without requiring there to be a causal relationship between them; their correlation can be explained by the fact that ‘Land Area’ is both a parent of ‘Stork Population’ and an ancestor of ‘Birth Rate’.\nThere are several structures that commonly appear appear in DAGs representing observational studies. In the rest of this article I want to focus on two of the most important – confounders and colliders – and explore how both of them can help to explain misleading results in medical research."
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#confounders",
    "href": "posts/observational-studies-and-causal-inference/index.html#confounders",
    "title": "Observational Studies and Causal Inference",
    "section": "Confounders",
    "text": "Confounders\nWhen we are investigating the effect of a variable X on another variable Y using an observational study, one of the most common ways we can be misled is by confounding8. This occurs when a third variable has an effect on both X and Y, which can lead to them being correlated even if there is not in fact a causal relationship between them. For instance, we could say that land area is a confounder when we are investigating the relationship between stork populations and birth rates, because an increase in land area leads to an increase in both variables. In a DAG, we can define a confounder to be a variable on a path between X and Y which is an ancestor of both of them9.\n\nIn the most extreme cases, confounding can actually mean that the true effect of X on Y is reversed, in a phenomenon known as Simpson’s paradox10. Robert Heaton gives an helpful illustration of this in his blog post ‘Making peace with Simpson’s Paradox’. He asks us to imagine a hypothetical illness with a specific drug treatment that causes faster recovery in higher dosages. This would seem quite straightforward to prove by collecting data on the amount of treatment different patients were given and the time it took them to recover. Then, however, he complicates the scenario by introducing a third variable – age. If older people tend to recover more slowly, and also tend to require higher dosages of the treatment drug to recover at all, an observational study that does not control for age may actually end up showing a negative correlation between drug dosage and recovery speed.\n\n\nSimulating and Plotting Data to Illustrate Robert Heaton’s Example of Simpson’s Paradox in R\nset.seed(31415) \ngroup_1_dosages &lt;- sample(1:20, 20, replace = TRUE)\ngroup_2_dosages &lt;- sample(21:40, 20, replace = TRUE)\ngroup_3_dosages &lt;- sample(41:60, 20, replace = TRUE)\nall_dosages &lt;- c(group_1_dosages, group_2_dosages, group_3_dosages)\ngroup_1_recovery_speeds &lt;- 1.2 * group_1_dosages + 100 + rnorm(20,0,5)\ngroup_2_recovery_speeds &lt;- 1.2 * group_2_dosages + 50 + rnorm(20,0,5)\ngroup_3_recovery_speeds &lt;- 1.2 * group_3_dosages + rnorm(20,0,5)\nall_recovery_speeds &lt;- c(group_1_recovery_speeds, group_2_recovery_speeds, group_3_recovery_speeds)\npar(mar=c(bottom=2.4, left=3, top=2.4, right=0.9), family=\"Roboto Slab\")\nplot(group_1_dosages, group_1_recovery_speeds, xlim=c(-3,72), ylim=c(36,120), bty=\"n\", xaxt=\"n\", yaxt=\"n\", pch=16, cex=2.4,  col=\"#009ed8\") \npoints(group_2_dosages, group_2_recovery_speeds, pch=17, cex=2.4,  col=\"#009ed8\")\npoints(group_3_dosages, group_3_recovery_speeds, pch=18, cex=2.4,  col=\"#009ed8\")\narrows(x0=0, y0=39, x1=72, y1=39, length=0.15, code=2, lwd=3, col=\"#808080\") \narrows(x0=0, y0=39, x1=0, y1=120, length=0.15, code=2, lwd=3, col=\"#808080\") \nmtext(\"Drug Dosage\", side=1, line=1, cex=1.8)\nmtext(\"Recovery Speed\", side=2, line=0, cex=1.8)\nabline(lm(group_1_recovery_speeds ~ group_1_dosages), col=\"#006a90\", lwd=4.8, lty=2) \nabline(lm(group_2_recovery_speeds ~ group_2_dosages), col=\"#006a90\", lwd=4.8, lty=2) \nabline(lm(group_3_recovery_speeds ~ group_3_dosages), col=\"#006a90\", lwd=4.8, lty=2) \nabline(lm(all_recovery_speeds ~ all_dosages), col=\"#ffc000\", lwd=4.8, lty=2)\nlegend(x=54, y=120, legend=c(\"20–39 Year Olds \", \"40–59 Year Olds \", \"60–79 Year Olds \"), pch=c(16, 17, 18), col=\"#009ed8\", cex=1.5)\nlegend(x=3, y=54, legend=c(\"Overall Line of Best Fit \", \"Age Group Lines of Best Fit \"), lty=2, col=c(\"#ffc000\",\"#006a90\"), lwd=4.2, cex=1.5)\n\n\n\n\n\n\n\n\n\nIn the simulated data plotted above, a higher dosage of the treatment drug corresponds with a faster recovery speed within each age category. However, when we combine data from all the age categories we find a negative correlation between drug dosage and recovery speed.\n\n\nFinding the Correlation Coefficient in R\ncorrelation &lt;- cor(all_dosages, all_recovery_speeds)\nprint(paste(\"Pearson Correlation Coefficient:\", correlation), quote=FALSE)\n\n\n[1] Pearson Correlation Coefficient: -0.745394350451033\n\n\nWhen we draw the DAG for this example, we can see that the variable ‘Age’ is acting as a counfounder, obscuring the true causal relationship between the ‘Drug Dosage’ and ‘Recovery Speed’ variables.\n\nSimpson’s paradox has been observed several times in the medical literature, perhaps most famously in a 1986 observational study by a group of urologists on the effectiveness of different types of procedure for the removal of kidney stones11. Wisely, the researchers recognised that stone size might be a confounding factor, so they split their data according to whether the original stone was less than two centimetres in diameter or not.\n\n  \n\nObserved Success Rate of Kidney Stone Removal Procedures\n\n\n\nSuccess Rate\n\n\nOpen Surgery\nPercutaneous Nephrolithotomy\n\n\nStone Size\nSmall\n≈ 93% \\(\\left( \\frac{81}{87} \\right)\\)\n≈ 87% \\(\\left( \\frac{234}{270} \\right)\\)\n\n\nLarge\n≈ 73% \\(\\left( \\frac{192}{263} \\right)\\)\n≈ 69% \\(\\left( \\frac{55}{80} \\right)\\)\n\n\nBoth\n≈ 78% \\(\\left( \\frac{273}{350} \\right)\\)\n≈ 83% \\(\\left( \\frac{289}{350} \\right)\\)\n\n\n\n\n\nThe urologists found that open surgery had been more successful than percutaneous nephrolithotomy in both small and large stones. But if they had not stratified their data by stone size, they would have obtained a higher overall success rate for percutaneous nephrolithotomy than for open surgery. The reason for this was that patients with small stones were more likely to be treated with percutaneous nephrolithotomy than patients with larger stones, and these were also the patients most likely to have a good result regardless of the procedure they underwent. So stone size influenced both the choice of treatment and the treatment outcome, and thus acted as a counfounder.\n\nThe size of a patient’s stone might have influenced their probability of having one treatment over another for any number of reasons. For instance, perhaps patients with smaller stones were more inclined to be concerned about the side effects of open surgery, or perhaps there was a longer waiting list for percutaneous nephrolithotomy that patients with larger stones tend to be less willing to tolerate. (These are just examples of possible explanations, not necessarily the true reasons that the size of kidney stones affected doctors and patients’ choice of treatment in the 1986 study.)"
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#colliders",
    "href": "posts/observational-studies-and-causal-inference/index.html#colliders",
    "title": "Observational Studies and Causal Inference",
    "section": "Colliders",
    "text": "Colliders\nAnother issue that can arise in observational studies is collider bias. When we are investigating the effect of X on Y, a collider is a variable lying on a path between X and Y where two causal arrows meet, meaning the collider has more than one parent.\n In observational medical studies, it is not uncommon that the variables we are investigating, X and Y, both have a causal effect on the probability that a patient is included in the study. In these cases, selection into the study is itself a collider variable.\nTo understand how collider bias can cause unrelated variables to appear to have a correlation, let’s imagine a hypothetical virus called ‘colliditis’. We’ll supose that the severity of disease experienced by people who catch colliditis is unrelated to the number of cigarettes they smoke in a day. For simplicity, we can assume that the average number of cigarettes of smoked a day and the severity of disease are uniformly distributed in the population of people who have caught colliditis. So if we plotted a random sample of one thousand people from the population of people with the viral infection, it would look like this.\n\n\nPlotting the Relationship Between Disease Severity and Average Number of Cigarettes Smoked in a Day in a Simulated Sample of People Infected With Colliditis in R\nset.seed(31415)\nn &lt;- 1000 \ninfected_sample &lt;- matrix(0, nrow=n, ncol=2) # we initialise an empty matrix to store the data for each person\nfor (i in 1:n) {\n  severity &lt;- runif(1, min=0, max=20) # we assign each person in the sample a random severity score between 0 and 20\n  cigarettes &lt;- runif(1,0,20) # we assign each person in the sample a random average number of cigarettes smoked per day between 0 and 20\n  infected_sample[i,1] &lt;- severity\n  infected_sample[i,2] &lt;- cigarettes\n}\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") \nplot(infected_sample[,1], infected_sample[,2], pch=4, cex=1.2, lwd=2.4, col=\"#808080\",\n     xlab=\"Colliditis Severity Score\", ylab=\"Average Number of Cigarettes Smoked in a Day\",\n     cex.lab=1.5, cex.axis=1.5)\nabline(lm(infected_sample[,1] ~ infected_sample[,2]), lwd=4.8, col=\"#009ed8\",) # we plot the line of best fit\n\n\n\n\n\n\n\n\n\nNow suppose that out of this sample, 90% the people with a colliditis severity score ≥15 are in hospital, and of those with a colliditis severity score &lt;15, those who smoke ≥10 cigarettes a day on average have a 60% chance of being in hospital, compared to a 10% chance for those who smoke &lt;10 cigarettes a day on average. If we plot cigarettes vs colliditis severity of only those in our sample who are hospitalised, we see an interesting result.\n\n\nPlotting the Relationship Between Disease Severity and Average Number of Cigarettes Smoked in a Day in a Simulated Sample of Hospitalised People Infected With Colliditis in R\nset.seed(31415)\nhospital_infected_sample &lt;- matrix(rep(NA, 2*n), nrow=n, ncol=2) \nfor (i in 1:n) {\n  dice_roll &lt;- runif(1, min=0, max=1) # we generate random number between 0 and 1\n  person_i &lt;- infected_sample[i,]\n  if(person_i[1] &gt;= 15 && dice_roll &lt;= 0.9) { # these people have a 90% chance of being in hospital\n    hospital_infected_sample[i,] &lt;- person_i\n  } else if(person_i[2] &gt;= 10 && dice_roll &lt;= 0.6) { # these people have a 60% chance of being in hospital\n    hospital_infected_sample[i,] &lt;- person_i\n  } else if(person_i[2] &lt; 10 && dice_roll &lt;= 0.1) { # others have a 10% chance of being in hospital\n    hospital_infected_sample[i,] &lt;- person_i\n  }\n}\n# Now we remove the people not in hospital from our sample:\nhospital_infected_sample &lt;- na.omit(hospital_infected_sample)\n# And plot the result:\npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\") \nplot(hospital_infected_sample[,1], hospital_infected_sample[,2], pch=4, cex=1.2, lwd=2.4, col=\"#808080\",\n     xlab=\"Colliditis Severity Score\", ylab=\"Average Number of Cigarettes Smoked in a Day\",\n     cex.lab=1.5, cex.axis=1.5)\nabline(lm(hospital_infected_sample[,1] ~ hospital_infected_sample[,2]), lwd=4.8, col=\"#009ed8\",) # we plot the line of best fit\n\n\n\n\n\n\n\n\n\nIf we took the hospitalised population as representative of the general population we would find that there is a negative relationship between average number of cigarettes smoked in a day and severity of colliditis. But we would be failing to account for the fact that having a severe case of colliditis, and smoking a large number of cigarettes per day are both factors that make you more likely to be in hospital. So if you are in hospital with a low colliditis score, you are more likely to smoke a lot of cigarettes than if you are in hospital with a high colliditis score, since there must be some other reason for your hospitalisation than colliditis, and this reason may be smoking.\nThe DAG for this example would show that ‘Hospitalisation’ is a collider, and that there is no true causal relationship between the variables ‘Smoking’ and ‘Severity’.\n\nFor an example of collider bias in the medical literature, let’s turn to a case-control study of hospital patients with and without pancreatic cancer published in 198112, which found a strong association between patients’ coffee consumption and their likelihood of having developed pancreatic cancer.\n\n  \n\nObserved Coffee Consumption in Patients With and Without Pancreatic Cancer\n\n\n\nCoffee Consumption\n\n\nPecentage That Drink 0 Cups Per Day\nPecentage That Drink 1 or 2 Cups Per Day\nPecentage That Drink 3+ Cups Per Day\n\n\nDiagnosis\nPancreatic Cancer\n≈ 5% \\(\\left( \\frac{20}{367} \\right)\\)\n≈ 42% \\(\\left( \\frac{153}{367} \\right)\\)\n≈ 53% \\(\\left( \\frac{194}{367} \\right)\\)\n\n\nNot Pacratic Cancer\n≈ 14% \\(\\left( \\frac{88}{643} \\right)\\)\n≈ 42% \\(\\left( \\frac{271}{643} \\right)\\)\n≈ 44% \\(\\left( \\frac{284}{643} \\right)\\)\n\n\n\n\n\nIn the decades since this study was published, several large prospective cohort studies have found no significant relationship between coffee consumption and pancreatic cancer risk13 14 15, suggesting that the original study may have had a flaw in its design. What could it have been?\nIn his seminal epidemiology textbook16, Leon Gordis points out the control group for this study – patients without pancreatic cancer – was chosen in an unusual way. When a patient who did have pancreatic cancer agreed to answer a survey about their dietary habits, control patients were selected by asking the doctor responsible for their care to identify other patients of theirs without the condition. Since these doctors were often gastroenterologists, patients with gastrointestinal disorders were over-represented in the control group. These patients tended to drink less coffee than the average member of the population, either because it exacerbated their symptoms or because they had been advised to cut down their intake by their doctors.\n\nAs a result of the atypical coffee consumption in the control group, the results of this observational study are hard to interpret. This is because even if it was the case that there was no causal relationship between coffee consumption and pancreatic cancer, we would still expect to see an association between them due to the fact that people with GI disorders were more likely to be chosen as controls than other patients. We can show this in a DAG in which ‘Selection into Study’ is a collider."
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#further-reading",
    "href": "posts/observational-studies-and-causal-inference/index.html#further-reading",
    "title": "Observational Studies and Causal Inference",
    "section": "Further Reading",
    "text": "Further Reading\n\nAn introduction to Causal inference – a blog post by Fabian Dablander\nKidney Stones & Simpson’s Paradox – a blog post by Amol Kulkarni"
  },
  {
    "objectID": "posts/observational-studies-and-causal-inference/index.html#footnotes",
    "href": "posts/observational-studies-and-causal-inference/index.html#footnotes",
    "title": "Observational Studies and Causal Inference",
    "section": "References",
    "text": "References\n\n\nJonathan Wood, “Life of a revolutionary,” OxSciBlog, November 11, 2009.↩︎\n“Study designs,” Centre for Evidence-Based Medicine, May 29, 2020.↩︎\nRobert Matthews, “Storks Deliver Babies (p=0.008),” Teaching Statistics 22, no. 2 (June 2000): 36–38.↩︎\nMartin Bland, An Introduction to Medical Statistics. 4th ed. Oxford: Oxford University Press, 2015.↩︎\nJudea Pearl and Dana Mackenzie, The Book of Why: The New Science of Cause and Effects. London: Penguin Books, 2019.↩︎\nSander Greenland, Judea Pearl, and James Robins, “Causal Diagrams for Epidemiologic Research,” Epidemiology 10, no. 1 (January 1999): 37–48.↩︎\nJudea Pearl, Madelyn Glymour, and Nicholas Jewell, Causal Inference in Statistics: A Primer. New York: Wiley, 2016.↩︎\nJeffrey Aronson, Clare Bankhead, and David Nunan, “Confounding,” Catalogue of Bias, 2018.↩︎\nRicardo Silva, “Causality,” (lecture, Advanced Tutorial Lecture Series on Machine Learning, Cambridge, November 9, 2006).↩︎\nSteven Julious and Mark Mullee, “Confounding and Simpson’s Paradox,” BMJ 309, no. 6968 (December 1994): 1480–1481.↩︎\nClive Charig et al., “Comparison of treatment of renal calculi by open surgery, percutaneous nephrolithotomy, and extracorporeal shockwave lithotripsy,” British Medical Journal (Clinical Research Edition) 292, no. 6524 (March 1986): 879–882.↩︎\nBrian MacMahon et al., “Coffee and cancer of the pancreas,” The New England Journal of Medicine 304, no. 11 (March 1981): 630–633.↩︎\nSimak Bidel et al., “Coffee consumption and risk of gastric and pancreatic cancer – A prospective cohort study,” International Journal of Cancer 312, no.7 (April 2012): 1651–1659.↩︎\nKristin Guertin et al., “A prospective study of coffee intake and pancreatic cancer: results from the NIH-AARP Diet and Health Study,” British Journal of Cancer 113, no. 7 (September 2015): 1081–1085.↩︎\nCharlie Zhou et al., “Coffee and pancreatic cancer risk among never-smokers in the UK prospective Million Women Study,” International Journal of Cancer 145 no. 6 (September 2019); 1484–1492.↩︎\nLeon Gordis, Epidemiology. 4th ed. Philadelphia: Saunders, 2008.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Censored Data and Kaplan-Meier Curves\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic Testing and Bayes’ Theorem\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservational Studies and Causal Inference\n\n\n\n\n\n\n\nNo matching items"
  }
]