[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a maths graduate who’s very interested in the many applications of mathematics and statistics to medicine, and I decided to start this blog as a way of challenging myself to keep learning new things (as well as deepening my understanding of areas I already have some knowledge of). It’s a place where I can practise writing expository articles that explore various topics in ‘medical maths’ using diagrams and R code. Thank you for reading – I hope you find my explanations interesting and/or useful!\n(If you notice an error or have any other feedback, please feel free to contact me at: medmaths@protonmail.com)\nThis site is built with Quarto and hosted via Github Pages."
  },
  {
    "objectID": "about.html#biostatsepidemiologymaths-adjacent-blogs-i-enjoy",
    "href": "about.html#biostatsepidemiologymaths-adjacent-blogs-i-enjoy",
    "title": "About",
    "section": "Biostats/Epidemiology/Maths Adjacent Blogs I Enjoy",
    "text": "Biostats/Epidemiology/Maths Adjacent Blogs I Enjoy\n\nLive Free or Dichotomize by Lucy D’Agostino McGowan and Nick Strayer\nPosting completely at random by Cameron Patrick\nProbably Overthinking It by Allen Downey\nTechnical Fridays by Harshit Kumar\nVariance Explained by David Robinson"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis blog post was written solely for informational and educational purposes, and does not constitute medical advice. If you have any health concerns, please consult a qualified medical professional.\nIf I am screened for a disease and receive a positive test result, how worried should I be? What are the chances it was a false positive? In this post I’ll discuss why medical test results aren’t always as straightforward as they might seem, and how an equation first discovered over 200 years ago can help us to understand them better."
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#sensitivity-and-specificity",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#sensitivity-and-specificity",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\nWhen interpreting a diagnostic test result, two of most important pieces of information we need to know are the sensitivity and specificity of the test. Coined in 1947 by Jacob Yerushalmy1, these terms refer respectively to the probability that a diagnostic test will correctly identify someone with a disease, and the probability that it will accurately distinguish those without the condition.\nSteven McGee gives a helpful illustration of these concepts in his book Evidence-Based Physical Diagnosis2, using a hypothetical experiment. He asks us to imagine that a specific physical examination finding – a holosystolic heart murmur – is assessed in a group of 42 people with a particular type of valvular heart disease and a group of 58 people without the condition, producing the following results.\n\n  \n\nPhysical Examination Findings in Patients With and Without Tricuspid Regurgitation\n\n\n\nValvular Heart Disease Status\n\n\nPatients With Tricuspid Regurgitation\nPatients Without Tricuspid Regurgitation\n\n\nPhysical Examination Finding\nHolosystolic Murmur Present\n22\n3\n\n\nHolosystolic Murmur Absent\n20\n55\n\n\n\n\n\nThe sensitivity of a test for a disease is the proportion of patients who have the condition and test positive. To calculate it we can divide the number of true positives by the sum of the number of true positives and the number of false negatives.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(\\text{FP}\\) denotes the number of false positives.\n\\(\\text{FN}\\) denotes the number of false negatives.\n\\(\\text{TP}\\) denotes the number of true positives.\n\\(\\text{TN}\\) denotes the number of true negatives.\n\n\n\nFor the example above this gives us a specificity of\n\\[\n\\text{Sensitivity}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}=\\frac{22}{22+20}=\\frac{22}{42}\\approx0.52,\n\\] equivalent to around 52%. We can find the specificity – the proportion of patients without the disease who correctly test negative – similarly;\n\\[\n\\text{Specificity}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}=\\frac{22}{55+3}=\\frac{55}{58}\\approx0.95.\n\\]"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#the-base-rate-fallacy",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#the-base-rate-fallacy",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "The Base Rate Fallacy",
    "text": "The Base Rate Fallacy\nNow we’re clear on the definitions of sensitivity and specificity, let’s look at a common way in which these values can be misinterpreted. Imagine there is a medical condition, let’s call it Bayesitis, with a prevalence of 5% – meaning it affects one in 20 people. Now suppose that a blood test for Bayesitis is found to have a sensitivity of 99% and a specificity of 85%. If a random member of the population has their blood tested and receives a positive result, what is the probability that it is a false positive and they don’t actually have the disease?\nWe might intuitively assume that the answer is 15% since, according to the definition of specificity, this is the proportion of patients without Bayesitis who will test positive incorrectly. Unfortunately, by making this assumption, we are falling into the trap of the base rate fallacy.\nLet’s see if we can get a ballpark figure for the true probability by thinking through what would happen if we were to screen a representative sample of 20 people for Bayesitis. We’ll assume that one person in our sample – 5% – has the disease. Since the sensitivity of our blood test is 99% this person is very likely to test positive, giving us one true positive. Now, what about the 19 people in our sample who do not have the disease? Referring to the specificity of our blood test, we can expect it to correctly identify 85% percent of them, giving us 16.15 true negatives. Since we can’t have 0.15 of a person, let’s round this to 16, which leaves us with three false positives.\n\nSo three out of four positive test results from our representative sample are false, meaning that if someone from our sample receives a positive test result, the probability that they don’t actually have Bayesitis is 75%. This is far enough from our initial guess of 15% for us to know that we must have made an error in our reasoning.\nOur problem was that we failed to take into account the population prevalence, or base rate, of Bayesitis. To see the importance of the base rate, consider a group of people in which 0% have the condition – in a population like this, there is a 100% probability that any positive test results are false.\nClearly, the sensitivity and specificity of a test aren’t on their own sufficient for us to calculate the probability that any particular positive test result is false. How, then, do we find the exact value of this probability?"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#bayes-to-the-rescue",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#bayes-to-the-rescue",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Bayes to the Rescue",
    "text": "Bayes to the Rescue\nThe answer, of course, is Bayes’ Theorem. Discovered by Thomas Bayes in the 1740s and given its modern form by Pierre Simon Laplace in the 1810s3, this famous theorem states that for any two events \\(A\\) and \\(B\\), we can get the conditional probability of \\(A\\) given \\(B\\) from the conditional probability of \\(B\\) given \\(A\\) using the formula\n\\[\n\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)},\n\\] where \\(\\mathbb{P}(A)\\) and \\(\\mathbb{P}(B)\\) are the marginal (unconditional) probabilities of \\(A\\) and \\(B\\) respectively.\nIn our example, \\(A\\) corresponds to the event that the person who had their blood tested actually has Bayesitis, and \\(B\\) corresponds to the event that they test positive for the disease. When we committed the base rate fallacy, we made the mistake of assuming that \\(\\mathbb{P}(A|B)\\) was equivalent to \\(\\mathbb{P}(B|A)\\), and we failed to consider the marginal probabilities \\(\\mathbb{P}(A)\\) and \\(\\mathbb{P}(B)\\). Let’s try to work out the probability that their test result was a false positive again, this time using Bayes’ theorem.\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(D^{-}\\) denotes not having the disease.\n\\(D^{+}\\) denotes having the disease.\n\\(T^{-}\\) denotes testing negative.\n\\(T^{+}\\) denotes testing positive\n\n\n\nWe want to find\n\\[\n\\mathbb{P}(D^{-}|T^{+})=\\frac{\\mathbb{P}(T^{+}|D^{-})\\mathbb{P}(D^{-})}{\\mathbb{P}(T^{+})},\n\\]\nWe know that \\(\\mathbb{P}(D^{-})=0.95\\), equivalent to 95%, since the prevalence of Bayesitis in the population is 95%. We also know \\(\\mathbb{P}(T^{+}|D^{-})=0.15\\), as we defined our imaginary test to be 85% accurate. So to find the probability of a positive test result being false we just need to find the denominator of the fraction above; \\(\\mathbb{P}(T^{+})\\), the overall probability of testing positive.\nWe can write\n\\[\n\\begin{split}\n\\mathbb{P}(T^{+})&=\\mathbb{P}(T^{+}\\cap(D^{+}\\cup D^{-}))\n\\\\&=\\mathbb{P}(T^{+}\\cap D^{+})+\\mathbb{P}(T^{+}\\cap D^{-})\n\\\\&=\\mathbb{P}(T^{+}|D^{+})\\mathbb{P}(D^{+})+\\mathbb{P}(T^{+}|D^{-})\\mathbb{P}(D^{-}),\n\\end{split}\n\\] using the fact that \\(D^{+}\\) and \\(D^{-}\\) are collectively exhaustive and mutually exclusive events, as well as the definition of conditional probability, which we can rearrange to get that \\(\\mathbb{P}(A\\cap B)=\\mathbb{P}(A|B)\\mathbb{P}(B)\\).\nNow, to work out \\(\\mathbb{P}(T^{+})\\), we can use our knowledge of the sensitivity and specificity of our blood test and the prevalence of Bayesitis to subsitute in the values \\(\\mathbb{P}(T^{+}|D^{+})=0.99\\), \\(\\mathbb{P}(D)=0.05\\), \\(\\mathbb{P}(T^{+}|D^{-})=0.15\\), and \\(\\mathbb{P}(D^{-})=0.95\\), finding that\n\\[\n\\mathbb{P}(T^{+})=0.99\\times 0.05 + 0.15\\times 0.95 = 0.192.\n\\] Finally, we can calculate that\n\\[\n\\mathbb{P}(D^{-}|T^{+})=\\frac{0.15\\times0.95}{0.192}=0.7421875,\n\\] which is pretty close to the estimate we got from our thought experiment involving 20 people!\nTo check our answer, we can use R to simulate repeatedly sampling 20 people from a population of 1 million people with a 5% prevalence of Bayesitis.\n\n\nR Code\n# Because we will be using randomly generated numbers, we first set a seed to ensure our results are reproducible:\nset.seed(31415926) \n# We now set up some variables:\npop_size &lt;- 1000000\nbayesitis_prevalence &lt;- 0.05\nbayesitis_pop_size &lt;- pop_size * bayesitis_prevalence # we calculate 5% of the population to find the number of people that have Bayesitis\n# Now imagine that each person in the population is assigned a number randomly pick 5% of these numbers to designate the people that have Bayesitis\ntotal_pop &lt;- 1:pop_size # we create a vector made up of the numbers from 1 to 1,000,000\nbayesitis_pop &lt;- sample(total_pop, size=bayesitis_pop_size, replace=FALSE) # we randomly samples 5% of the entries from our total_pop vector\n# Let's create a logical vector (one that only contains TRUE and FALSE values) to represent the Bayesitis status of every member of the population:\nbayesitis_statuses &lt;- rep(FALSE, times=pop_size) # we initialise a vector the length of the population where every entry is FALSE\nbayesitis_statuses[bayesitis_pop] &lt;- TRUE # we change the entries corresponding to the people with Byesitis to TRUE\n# We can now set up our simulations: \nnum_sims &lt;- 300\nfalse_pos_props &lt;- rep(NA, times=num_sims) # we initialise a vector to store the proportion of false positives we get from each simulation \n# Now let's repeatedly pick 20 random people from the population and simulate screening them for Bayesitis:\nsamp_size &lt;- 20\nfor (n in 1:num_sims) {\n  rand_samp &lt;- sample(total_pop, size=samp_size, replace=FALSE)\n  # Let's set up variables to store the number of true and false positive test results:\n  num_true_pos &lt;- 0\n  num_false_pos &lt;- 0\n  rand_nums &lt;- runif(samp_size, min=0, max=1) # we generate 20 random numbers between 0 and 1, so that everyone in the sample gets a correct/incorrect\n                                              # test result with a probability according to the sensitivity and specificty of our test\n  for (i in 1:samp_size) {\n    true_status &lt;- bayesitis_statuses[rand_samp[i]]\n    if (rand_nums[i] &lt;= 0.99 & true_status == TRUE) { # if the person does have Bayesitis they get an accurate positive result with a probability of 99%\n      num_true_pos &lt;- num_true_pos + 1\n    } else if (rand_nums[i] &gt; 0.85 & true_status == FALSE) { # if they don't they get a false positive result with a probability of 15%\n      num_false_pos &lt;- num_false_pos + 1\n    }\n  }\n  total_pos &lt;- num_true_pos + num_false_pos\n  if (total_pos != 0) { # if there are no positive test results, true or false, the proportion of positive test results that are false is undefined\n    false_pos_props[n] &lt;- num_false_pos / total_pos\n  }  \n}\n# Now let's see how the average proportion of positive test results that are false changes as the number of simulations we've done increases:\nprops_for_avg &lt;- false_pos_props[!is.na(false_pos_props)] # we remove the undefined values from our vector of empirical false positive proportions\ncum_avg &lt;- cumsum(props_for_avg) / seq_along(props_for_avg) # we calculate a cumulative average as the number of simulations increases\n# \npar(mar=c(bottom=4.2, left=5.1, top=1.8, right=0.9), family=\"Roboto Slab\")\nplot(cum_avg, ylim=c(0.5,1), xlab=\"Number of Simulations\", ylab=\"Average False Positive Proportion\", type=\"l\", col=\"#808080\", lwd=4.2, cex.lab=1.5, cex.axis=1.5)\nabline(h=0.7421875, col=\"#009ed8\", lty=1, lwd=4.2)\nlegend(\"topright\", legend=c(\"Calculated Probability\"), col=\"#009ed8\", lty=1, lwd=9, cex=1.2)\n\n\n\n\n\n\n\n\n\nReassuringly, as we do more simulations, the average proportion of positive test results that are false appears to converge to our calculated value of \\(\\mathbb{P}(D^{-}|T^{+})\\)."
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#further-reading",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#further-reading",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "Further Reading",
    "text": "Further Reading\n\nBayes theorem and likelihood ratios for diagnostic tests – a blog post by Stathis Kamperis about likelihood ratios\nAn Interactive Fagan Nomogram – a blog post by Carlos Scheidegger about the Fagan nomogram"
  },
  {
    "objectID": "posts/diagnostic-testing-and-bayes-theorem/index.html#footnotes",
    "href": "posts/diagnostic-testing-and-bayes-theorem/index.html#footnotes",
    "title": "Diagnostic Testing and Bayes’ Theorem",
    "section": "References",
    "text": "References\n\n\nJacob Yerushalmy, “Statistical problems in assessing methods of medical diagnosis with special reference to x-ray techniques,” Public Health Reports 62, no. 2 (1947): 1432–1439.↩︎\nSteven McGee, Evidence-Based Physical Diagnosis. Philadelphia: Saunders, 2012.↩︎\nSharon Bertsch McGrayne, The Theory That Would Not Die. New Haven: Yale University Press, 2011.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nJanuary 30, 2025\n\n\nDiagnostic Testing and Bayes’ Theorem\n\n\n\n\n\nNo matching items"
  }
]